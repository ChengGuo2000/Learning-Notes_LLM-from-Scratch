{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c12c061",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) Tokenizer From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3dfb58",
   "metadata": {},
   "source": [
    "## Bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4424c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'So far, I had')\n"
     ]
    }
   ],
   "source": [
    "# Bits and bytes\n",
    "text = \"So far, I had\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cb5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 111, 32, 102, 97, 114, 44, 32, 73, 32, 104, 97, 100]\n"
     ]
    }
   ],
   "source": [
    "# Get byte values\n",
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a20f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 13\n",
      "Number of token IDs: 13\n"
     ]
    }
   ],
   "source": [
    "# This is creating an ID for each character, not each word\n",
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc20e2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2396, 1290, 11, 314, 550]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how it is encoded with tiktoken\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"So far, I had\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00aa7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n"
     ]
    }
   ],
   "source": [
    "# check the first 256 single-character tokens\n",
    "for i in range(260):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9977616b",
   "metadata": {},
   "source": [
    "## A simple BPE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad33bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with 'Ġ'\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # Optionally, ensure 'Ġ' is included if it is relevant to your text processing\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  # No more pairs to merge. Stopping training.\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Split text into tokens, keeping newlines intact\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  # Ensure '\\n' is treated as a separate token\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # Add 'Ġ' to words that follow a space or newline\n",
    "            else:\n",
    "                tokens.append(word)  # Handle first word or standalone '\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    # Uncomment for educational purposes:\n",
    "                    # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # Skip the next token as it's merged\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # Replace 'Ġ' with a space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d32fc",
   "metadata": {},
   "source": [
    "## Simple BPE implementation walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4de3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training text\n",
    "with open(\"pit-and-pendulum.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31b9dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f', 128: '\\x80', 129: '\\x81', 130: '\\x82', 131: '\\x83', 132: '\\x84', 133: '\\x85', 134: '\\x86', 135: '\\x87', 136: '\\x88', 137: '\\x89', 138: '\\x8a', 139: '\\x8b', 140: '\\x8c', 141: '\\x8d', 142: '\\x8e', 143: '\\x8f', 144: '\\x90', 145: '\\x91', 146: '\\x92', 147: '\\x93', 148: '\\x94', 149: '\\x95', 150: '\\x96', 151: '\\x97', 152: '\\x98', 153: '\\x99', 154: '\\x9a', 155: '\\x9b', 156: '\\x9c', 157: '\\x9d', 158: '\\x9e', 159: '\\x9f', 160: '\\xa0', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 173: '\\xad', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 256: 'Ġ', 257: '—', 258: 'eĠ', 259: 'th', 260: 'dĠ', 261: 'tĠ', 262: 'Ġth', 263: 'in', 264: 'Ġa', 265: 'sĠ', 266: 'er', 267: 'yĠ', 268: 'on', 269: 'en', 270: 'ĠtheĠ', 271: 'edĠ', 272: 'of', 273: 'or', 274: ',Ġ', 275: '.Ġ', 276: 're', 277: 'IĠ', 278: 'ou', 279: 'Ġm', 280: 'ha', 281: 'to', 282: 'ndĠ', 283: 'es', 284: 'st', 285: 'll', 286: 'ing', 287: 'ar', 288: 'it', 289: 'an', 290: 'wa', 291: ',Ġa', 292: 'at', 293: 'Ġs', 294: 'atĠ', 295: 'ion', 296: 'theĠ', 297: 'no', 298: 'om', 299: 'for', 300: 'ch', 301: 'ofĠtheĠ', 302: 'ofĠ', 303: 'ĠmyĠ', 304: 'wh', 305: 'el', 306: 'ea', 307: 'ir', 308: 'ed', 309: 'toĠ', 310: 'hadĠ', 311: 'wi', 312: ',ĠandĠ', 313: 'Th', 314: 'ent', 315: 'is', 316: 'em', 317: 'up', 318: 'ingĠ', 319: 'itĠ', 320: 'al', 321: 'inĠ', 322: 'be', 323: 'gh', 324: 'ĠaĠ', 325: 'ereĠ', 326: 'ra', 327: 'os', 328: 'ver', 329: 'un', 330: 'with', 331: '.ĠTh', 332: 'ur', 333: 'lyĠ', 334: '.ĠIĠ', 335: 'enĠ', 336: 'ldĠ', 337: 'myĠ', 338: 'ter', 339: 'ri', 340: 'ow', 341: 'wasĠ', 342: 'ce', 343: 'res', 344: 'le', 345: 'im', 346: 'per', 347: 'whi', 348: 'veĠ', 349: 'ct', 350: 'ul', 351: 'ĠthatĠ', 352: 'upon', 353: 'us', 354: 'utĠ', 355: 'ca', 356: 'con', 357: 'ro', 358: 'llĠ', 359: ',ĠIĠ', 360: 'chĠ', 361: 'meĠ', 362: 'la', 363: 'ouldĠ', 364: 'ear', 365: 'um', 366: 'rea', 367: '.ĠI', 368: 'dea', 369: 'me', 370: 'fr', 371: 'sa', 372: '\\n\\n', 373: 'itsĠ', 374: 'ceĠ', 375: 'ionĠ', 376: 'ough', 377: 'ly', 378: 'ĠandĠ', 379: 'se', 380: 'notĠ', 381: 'orĠ', 382: 'from', 383: 'isĠ', 384: 'li', 385: 'andĠ', 386: 'entĠ', 387: '.\\n\\n', 388: 'leĠ', 389: 'ag', 390: 'ess', 391: 'lo', 392: 'di', 393: 'butĠ', 394: '!Ġ', 395: 'nĠ', 396: 'Ġp', 397: 'des', 398: 'thatĠ', 399: '.ĠTheĠ', 400: 'ain', 401: 'ĠatĠ', 402: 'qu', 403: 'de', 404: ';Ġ', 405: 'vi', 406: 'whichĠ', 407: '.ĠA', 408: 'Ġf', 409: 'ge', 410: 'erĠ', 411: 'haveĠ', 412: 'alĠ', 413: 'ex', 414: 'su', 415: 'ck', 416: 'ong', 417: 'esĠ', 418: 'hatĠ', 419: 'fe', 420: 'edĠtheĠ', 421: 'ep', 422: 'si', 423: 'stĠ', 424: 'sh', 425: 'sp', 426: 'fi', 427: 'essĠ', 428: 'str', 429: 'beenĠ', 430: 'len', 431: 'oundĠ', 432: 'was', 433: 'ol', 434: 'ab', 435: 'we', 436: 'inĠtheĠ', 437: 'ghtĠ', 438: 'omeĠ', 439: 'gl', 440: 'toĠtheĠ', 441: 'thĠ', 442: 'ut', 443: 'ci', 444: 'fir', 445: 'ever', 446: 'beĠ', 447: 'asĠ', 448: 'war', 449: 'couldĠ', 450: 'ofĠmyĠ', 451: 'aĠ', 452: 'fa', 453: '.ĠItĠ', 454: 'fĠ', 455: 'od', 456: 'whatĠ', 457: 'sti', 458: 'hor', 459: 'eyĠ', 460: 'oĠ', 461: 'ĠthereĠ', 462: 'ous', 463: 'pa', 464: 'now', 465: 'long', 466: 'leng', 467: 'tedĠ', 468: 'toĠmyĠ', 469: 'saw', 470: 'pp', 471: 'wall', 472: 'over', 473: 'rem', 474: 'il', 475: 'cou', 476: 'Ġar', 477: 'end', 478: 'ey', 479: 'wereĠ', 480: 'withĠ', 481: 'anĠ', 482: 'ation', 483: 'IĠhadĠ', 484: 'fromĠtheĠ', 485: 'anyĠ', 486: 'ĠthisĠ', 487: 'nd', 488: 'edĠmyĠ', 489: ',ĠtheĠ', 490: 'seem', 491: 'oneĠ', 492: 'dd', 493: 'and', 494: 'oreĠ', 495: 'cameĠ', 496: 'ofĠaĠ', 497: 'form', 498: 'though', 499: 'geĠ', 500: 'ma', 501: 'op', 502: 'ob', 503: 'ctĠ', 504: 'min', 505: 'ss', 506: 'y,Ġ', 507: 'nessĠ', 508: 'erv', 509: 'ofĠs', 510: 'edĠtoĠ', 511: 'hadĠbeenĠ', 512: 'byĠ', 513: 'ofĠa', 514: 'et', 515: 'eir', 516: 'pres', 517: 'ness', 518: 'ousĠ', 519: 'ti', 520: 'wouldĠ', 521: 'ĠasĠ', 522: 'hea', 523: 'ish', 524: 'kĠ', 525: 'wo', 526: 'pr', 527: 'mi', 528: 'ef', 529: 'onĠ', 530: 'own', 531: 'sel', 532: 'my', 533: 'ationĠ', 534: 'etĠ', 535: 'mo', 536: 'ort', 537: 'e,Ġ', 538: 'oseĠ', 539: 'too', 540: 'horr', 541: 'uponĠtheĠ', 542: 'ta', 543: 'cha', 544: 'keĠ', 545: 'desc', 546: 'seĠ', 547: '?Ġ', 548: 'va', 549: 'elyĠ', 550: 'uponĠmyĠ', 551: 'cl', 552: 'eltĠ', 553: 'withĠtheĠ', 554: ';ĠbutĠ', 555: 'sĠofĠtheĠ', 556: 'ĠmeĠ', 557: 'ityĠ', 558: 'ure', 559: 'br', 560: 'ect', 561: 'ustĠ', 562: 'outĠ', 563: 'stru', 564: 'which', 565: 'da', 566: 'pl', 567: 'dist', 568: 'seemedĠ', 569: 'ateĠ', 570: 'hear', 571: 'bleĠ', 572: 'eseĠ', 573: 's—', 574: 'ev', 575: 'Ġtheir', 576: 'sĠofĠ', 577: 'ity', 578: 'der', 579: 'Ġan', 580: 'att', 581: 'ofĠtheĠs', 582: '.ĠW', 583: 'ĠaĠs', 584: 'ofĠm', 585: 'cond', 586: 'pro', 587: 'ed,Ġ', 588: 'com', 589: 'flo', 590: 'ful', 591: 'ba', 592: 'gre', 593: 'edĠme', 594: 'forĠ', 595: 'dun', 596: 'dunge', 597: 'cir', 598: 'circ', 599: 'pris', 600: 'Ġag', 601: 'toĠs', 602: 'oi', 603: 'how', 604: 'e—', 605: 'wor', 606: 'emp', 607: 'stillĠ', 608: 'lesĠ', 609: 'horror', 610: 'par', 611: 'firstĠ', 612: 'ostĠ', 613: 'noĠ', 614: 'length', 615: 'if', 616: 'intoĠ', 617: 'estĠ', 618: 'd,Ġ', 619: 'ButĠ', 620: 'dden', 621: 'edĠmeĠ', 622: 'coun', 623: 'all', 624: 'bl', 625: 'ce,Ġ', 626: 'es,Ġ', 627: 'fo', 628: 'ĠmanyĠ', 629: 'clos', 630: 'therĠ', 631: 'ext', 632: 'hop', 633: 'deathĠ', 634: ',ĠandĠIĠ', 635: 'feltĠ', 636: 'ac', 637: 'quis', 638: 'quisit', 639: 'ori', 640: 'oul', 641: 'peri', 642: 'Ġapp', 643: 'press', 644: 'ps', 645: '.ĠAtĠ', 646: 'fra', 647: 'longĠ', 648: 'befor', 649: 'weĠ', 650: 'brea', 651: 'ingĠtheĠ', 652: 'strug', 653: 'veryĠ', 654: 'Ġal', 655: 'heart', 656: 'Ġthr', 657: 'layĠ', 658: 'son', 659: 'tim', 660: 'ction', 661: 'dungeon', 662: 'well', 663: 'ed—', 664: 'lef', 665: 'our', 666: 'prison', 667: 'edĠandĠ', 668: 'sĠtheĠ', 669: 'selfĠ', 670: 'ĠanĠ', 671: 'pend', 672: 'lengthĠ', 673: 'lea', 674: 'drea', 675: 'death', 676: 'cesĠ', 677: 'edĠinĠ', 678: 'idea', 679: 'ĠofĠ', 680: 'onlyĠ', 681: 'forĠaĠ', 682: 'withĠaĠ', 683: 'ceed', 684: 'oment', 685: ',ĠthereĠ', 686: 'everyĠ', 687: 'inĠmyĠ', 688: ',ĠasĠ', 689: 'inĠa', 690: 'rus', 691: 'rush', 692: 'ber', 693: 'In', 694: 'been', 695: 'med', 696: 'ret', 697: ',ĠatĠ', 698: 'vel', 699: 'co', 700: 'ew', 701: '.\\n\\nA', 702: 'reg', 703: 'ĠsomeĠ', 704: 'whenĠ', 705: 'lat', 706: 'Ġac', 707: 'Ġmo', 708: 'out', 709: 'back', 710: 'uchĠ', 711: 'IĠcouldĠ', 712: '.ĠM', 713: 'pos', 714: 'step', 715: 'pe', 716: 'someĠ', 717: 'mightĠ', 718: 'wardĠ', 719: 'antĠ', 720: '.ĠF', 721: 'nowĠ', 722: 'sur', 723: 'pendul', 724: 'pendulum', 725: '.ĠButĠ', 726: 'band', 727: 'when', 728: 'wasĠtheĠ', 729: 'distin', 730: 'fter', 731: 'voi', 732: 'soul', 733: 'hap', 734: 'fromĠ', 735: 'anc', 736: 'bur', 737: 'eel', 738: 'bri', 739: 'ud', 740: 'uponĠ', 741: 'ens', 742: '.ĠIĠsaw', 743: 'ment', 744: '.ĠAndĠ', 745: 'vis', 746: 'spir', 747: 'whileĠ', 748: 'not', 749: 'thoughtĠ', 750: 'thereĠ', 751: 'call', 752: '.ĠThen', 753: 'fin', 754: '.ĠIn', 755: 's,Ġ', 756: 'so', 757: 'tw', 758: ',ĠthatĠ', 759: 'ph', 760: 'inter', 761: 'eĠwh', 762: 'notĠh', 763: 'fre', 764: 'fulĠ', 765: 'endea', 766: 'endeav', 767: 'ther', 768: ',Ġin', 769: 'down', 770: 'ĠofĠtheĠ', 771: '.ĠTheyĠ', 772: 'tion', 773: 'less', 774: 'Ġthrough', 775: 'lim', 776: 'ingĠmyĠ', 777: 'pre', 778: 'aĠs', 779: 'effor', 780: ',ĠIĠhadĠ', 781: 'ainĠ', 782: 'into', 783: 'deĠ', 784: 'ĠatĠtheĠ', 785: 'ĠtheseĠ', 786: 'Ġawa', 787: 'floor', 788: 'by', 789: 'edĠfor', 790: 'ght', 791: 'ward', 792: 'rat', 793: 'them', 794: 'leftĠ', 795: 'hour', 796: 'iedĠ', 797: 'ard', 798: 'ung', 799: 'pit', 800: 'serv', 801: 'hal', 802: ';ĠandĠ', 803: 'Ġme', 804: 'hum', 805: 'ution', 806: 'sĠa', 807: 'entlyĠ', 808: 'ore', 809: '.ĠY', 810: '!ĠIĠ', 811: 'bla', 812: 'edĠto', 813: 'ĠtheirĠ', 814: 'cont', 815: 'del', 816: 'ic', 817: 'swe', 818: 'beforeĠ', 819: 'fig', 820: 'thing', 821: 'dar', 822: 'sup', 823: 'still', 824: 'ni', 825: 'iver', 826: 'woon', 827: 'cons', 828: 'attemp', 829: 'mostĠ', 830: 'Ġse', 831: 'al,Ġ', 832: 'sten', 833: 'ĠtheyĠ', 834: 'low', 835: 'orsĠ', 836: 'glesĠ', 837: 'whenĠIĠ', 838: 'cess', 839: 'fer', 840: 'atĠtheĠ', 841: 'descent', 842: 'ueĠ', 843: 'Ġthough', 844: 'tr', 845: 'sib', 846: 'eyes', 847: 'thatĠIĠ', 848: 'hand', 849: 'edĠitĠ', 850: 'minut', 851: 'ĠaroundĠ', 852: 'fear', 853: '.ĠMyĠ', 854: 'Ġthen', 855: 'pass', 856: 'struggl', 857: 'breath', 858: 'ition', 859: 'TheĠ', 860: 'kn', 861: 'onĠtheĠ', 862: 'onceĠ', 863: 'lightĠ', 864: 'Ġsu', 865: 'odĠ', 866: 's.ĠIĠ', 867: 'wallsĠ', 868: 'ig', 869: 'eyesĠ', 870: 'ran', 871: 'ĠmoreĠ', 872: 'terĠ', 873: 'dou', 874: 'oc', 875: 'tain', 876: ',Ġhow', 877: 'sc', 878: 'circu', 879: 'ledĠ', 880: 'itĠwasĠ', 881: 'chan', 882: 'rap', 883: 'mentĠ', 884: 'riv', 885: 'ly—', 886: 'lin', 887: '.ĠFor', 888: 'atedĠ', 889: 'myselfĠ', 890: 'nerv', 891: 'ureĠ', 892: 'formedĠ', 893: 'iron', 894: 'observ', 895: 'ven', 896: 'steel', 897: 'oh', 898: 'ith', 899: 'bos', 900: 'bosom', 901: 'bandag', 902: '—theĠ', 903: 'enceĠ', 904: 'cent', 905: 'reach', 906: 'quisitori', 907: '.ĠThisĠ', 908: 'period', 909: 'Ġthan', 910: 'whichĠIĠ', 911: 'evenĠ', 912: 'mov', 913: 'dead', 914: 'shu', 915: 'shudd', 916: 'shudder', 917: 'near', 918: 'theyĠ', 919: 'spe', 920: 'mean', 921: 'head', 922: 'restĠ', 923: 'mustĠ', 924: 'e.ĠTheĠ', 925: 'fu', 926: 'justĠ', 927: 'nessĠofĠ', 928: 'Ġma', 929: 'yetĠ', 930: 'first', 931: 'enseĠ', 932: 'pression', 933: 'ishĠ', 934: 'interv', 935: 'hasĠ', 936: 'ind', 937: 'wil', 938: 'rest', 939: 'Ġat', 940: 'haveĠbeenĠ', 941: 'uredĠ', 942: 'sure', 943: 'itionĠ', 944: 'distinct', 945: 'hi', 946: 'nat', 947: 'bus', 948: 'eryĠ', 949: 'sudden', 950: 'ound', 951: 'ult', 952: 'ofĠitsĠ', 953: 'ĠtheĠm', 954: 't—', 955: 'tri', 956: 'ofĠall', 957: 'ome', 958: 'har', 959: 'ffer', 960: 'imag', 961: 'anceĠ', 962: 'obj', 963: 'ble', 964: ',ĠbutĠ', 965: 'grew', 966: 'dĠtheĠ', 967: 'oin', 968: 'didĠ', 969: 'knew', 970: 'ofĠtheseĠ', 971: 'pla', 972: 'nedĠ', 973: 'cell', 974: 'moreĠ', 975: 'Up', 976: 'star', 977: 'dire', 978: 'ed,ĠandĠ', 979: 'vid', 980: 'doub', 981: 'wasĠaĠ', 982: 'y—', 983: 'ertain', 984: 'iv', 985: 'eĠofĠtheĠ', 986: '.ĠIĠhadĠ', 987: 'dis', 988: '.ĠInĠ', 989: 'countedĠ', 990: 'fell', 991: 'ĠtoĠtheĠ', 992: 'ty', 993: 'two', 994: 'itt', 995: 'tyĠ', 996: 'ros', 997: 'forĠtheĠ', 998: 'fall', 999: ',ĠandĠtheĠ'}\n",
      "1000\n",
      "742\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the BPE tokenizer with a vocab size of 1000\n",
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special=\"<|endoftext|>\")\n",
    "print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))\n",
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd5eaa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 111, 408, 287, 44, 256, 73, 256, 280, 100]\n"
     ]
    }
   ],
   "source": [
    "# Test of encoding\n",
    "input_text = \"So far, I had\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0e333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 13\n",
      "Number of token IDs: 10\n"
     ]
    }
   ],
   "source": [
    "# Print length information\n",
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0235c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 111, 408, 287, 44, 256, 73, 256, 280, 100]\n",
      "So far, I had\n"
     ]
    }
   ],
   "source": [
    "# Test of decoding\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bec7b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 -> S\n",
      "111 -> o\n",
      "408 ->  f\n",
      "287 -> ar\n",
      "44 -> ,\n",
      "256 ->  \n",
      "73 -> I\n",
      "256 ->  \n",
      "280 -> ha\n",
      "100 -> d\n"
     ]
    }
   ],
   "source": [
    "# Better understanding of decoding\n",
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c11b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same text reproducible\n",
    "tokenizer.decode(tokenizer.encode(\"Hello everyone!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7d538",
   "metadata": {},
   "source": [
    "## An improved BPE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5da2d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizerImproved:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
    "        #  of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with \"Ġ\"\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # encoder.json is {token_str: id}; we want id->str and str->id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "    \n",
    "        # Must have GPT-2's printable newline character 'Ċ' (U+010A) at id 198\n",
    "        if \"Ċ\" not in self.inverse_vocab or self.inverse_vocab[\"Ċ\"] != 198:\n",
    "            raise KeyError(\"Vocabulary missing GPT-2 newline glyph 'Ċ' at id 198.\")\n",
    "    \n",
    "        # Must have <|endoftext|> at 50256\n",
    "        if \"<|endoftext|>\" not in self.inverse_vocab or self.inverse_vocab[\"<|endoftext|>\"] != 50256:\n",
    "            raise KeyError(\"Vocabulary missing <|endoftext|> at id 50256.\")\n",
    "    \n",
    "        # Provide a convenience alias for '\\n' -> 198\n",
    "        # Keep printable character 'Ċ' in vocab so BPE merges keep working\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            self.inverse_vocab[\"\\n\"] = self.inverse_vocab[\"Ċ\"]\n",
    "\n",
    "        if \"\\r\" not in self.inverse_vocab:\n",
    "            if 201 in self.vocab:\n",
    "                self.inverse_vocab[\"\\r\"] = 201\n",
    "            else:\n",
    "                raise KeyError(\"Vocabulary missing carriage return token at id 201.\")\n",
    "\n",
    "        # Load GPT-2 merges and store ranks\n",
    "        self.bpe_ranks = {}\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "    \n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                token1, *rest = line.strip().split()\n",
    "                if len(rest) != 1:\n",
    "                    continue\n",
    "                token2 = rest[0]\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    self.bpe_ranks[(token1, token2)] = rank\n",
    "                    rank += 1\n",
    "                else:\n",
    "                    # Safe to skip pairs whose symbols are not in vocab\n",
    "                    pass\n",
    "\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "    \n",
    "        # ---- This section is to mimic tiktoken in terms of allowed special tokens ----\n",
    "        specials_in_vocab = [\n",
    "            tok for tok in self.inverse_vocab\n",
    "            if tok.startswith(\"<|\") and tok.endswith(\"|>\")\n",
    "        ]\n",
    "        if allowed_special is None:\n",
    "            # Nothing is allowed\n",
    "            disallowed = [tok for tok in specials_in_vocab if tok in text]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "        else:\n",
    "            # Some spefic tokens are allowed (e.g., we use this for <|endoftext|>)\n",
    "            disallowed = [tok for tok in specials_in_vocab if tok in text and tok not in allowed_special]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "        # -----------------------------------------------------------------------------\n",
    "\n",
    "        token_ids = []\n",
    "        # If some specials are allowed, split around them and passthrough those ids\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            special_pattern = \"(\" + \"|\".join(\n",
    "                re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)\n",
    "            ) + \")\"\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # encode prefix normally\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # remainder to process normally\n",
    "    \n",
    "            # Extra guard for any other special literals left over\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "\n",
    "    \n",
    "        # ---- Newline and carriage return handling ----\n",
    "        tokens = []\n",
    "        parts = re.split(r'(\\r\\n|\\r|\\n)', text)\n",
    "        for part in parts:\n",
    "            if part == \"\":\n",
    "                continue\n",
    "            if part == \"\\r\\n\":\n",
    "                tokens.append(\"\\r\")\n",
    "                tokens.append(\"\\n\")\n",
    "                continue\n",
    "            if part == \"\\r\":\n",
    "                tokens.append(\"\\r\")\n",
    "                continue\n",
    "            if part == \"\\n\":\n",
    "                tokens.append(\"\\n\")\n",
    "                continue\n",
    "    \n",
    "            # Normal chunk without line breaks:\n",
    "            # - If spaces precede a word, prefix the first word with 'Ġ' and\n",
    "            #   add standalone 'Ġ' for additional spaces\n",
    "            # - If spaces trail the chunk (e.g., before a newline) add\n",
    "            #   standalone 'Ġ' tokens (tiktoken produces id 220 for 'Ġ')\n",
    "            pending_spaces = 0\n",
    "            for m in re.finditer(r'( +)|(\\S+)', part):\n",
    "                if m.group(1) is not None:\n",
    "                    pending_spaces += len(m.group(1))\n",
    "                else:\n",
    "                    word = m.group(2)\n",
    "                    if pending_spaces > 0:\n",
    "                        tokens.append(\"Ġ\" + word) # one leading space\n",
    "                        for _ in range(pending_spaces - 1):\n",
    "                            tokens.append(\"Ġ\")  # remaining spaces as standalone\n",
    "                        pending_spaces = 0\n",
    "                    else:\n",
    "                        tokens.append(word)\n",
    "            # Trailing spaces (no following word): add standalone 'Ġ' tokens\n",
    "            for _ in range(pending_spaces):\n",
    "                tokens.append(\"Ġ\")\n",
    "        # ---------------------------------------------------------------\n",
    "    \n",
    "        # Map tokens -> ids (BPE if needed)\n",
    "        for tok in tokens:\n",
    "            if tok in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[tok])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(tok))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        # If we haven't loaded OpenAI's GPT-2 merges, use my approach\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        # Uncomment for educational purposes:\n",
    "                        # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                        i += 2  # Skip the next token as it's merged\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "\n",
    "        # Otherwise, do GPT-2-style merging with the ranks:\n",
    "        # 1) Convert token_ids back to string \"symbols\" for each ID\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "\n",
    "        # Repeatedly merge all occurrences of the lowest-rank pair\n",
    "        while True:\n",
    "            # Collect all adjacent pairs\n",
    "            pairs = set(zip(symbols, symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Find the pair with the best (lowest) rank\n",
    "            min_rank = float(\"inf\")\n",
    "            bigram = None\n",
    "            for p in pairs:\n",
    "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
    "                if r < min_rank:\n",
    "                    min_rank = r\n",
    "                    bigram = p\n",
    "\n",
    "            # If no valid ranked pair is present, we're done\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            # Merge all occurrences of that pair\n",
    "            first, second = bigram\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # If we see (first, second) at position i, merge them\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                    new_symbols.append(first + second)  # merged symbol\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "\n",
    "        # Finally, convert merged symbols back to IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for tid in token_ids:\n",
    "            if tid not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {tid} not found in vocab.\")\n",
    "            tok = self.vocab[tid]\n",
    "\n",
    "            # Map GPT-2 special chars back to real chars\n",
    "            if tid == 198 or tok == \"\\n\":\n",
    "                out.append(\"\\n\")\n",
    "            elif tid == 201 or tok == \"\\r\":\n",
    "                out.append(\"\\r\")\n",
    "            elif tok.startswith(\"Ġ\"):\n",
    "                out.append(\" \" + tok[1:])\n",
    "            else:\n",
    "                out.append(tok)\n",
    "        return \"\".join(out)\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84bc1a9",
   "metadata": {},
   "source": [
    "## BPE implementation walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbc6b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training text\n",
    "with open(\"pit-and-pendulum.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be0c1113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f', 128: '\\x80', 129: '\\x81', 130: '\\x82', 131: '\\x83', 132: '\\x84', 133: '\\x85', 134: '\\x86', 135: '\\x87', 136: '\\x88', 137: '\\x89', 138: '\\x8a', 139: '\\x8b', 140: '\\x8c', 141: '\\x8d', 142: '\\x8e', 143: '\\x8f', 144: '\\x90', 145: '\\x91', 146: '\\x92', 147: '\\x93', 148: '\\x94', 149: '\\x95', 150: '\\x96', 151: '\\x97', 152: '\\x98', 153: '\\x99', 154: '\\x9a', 155: '\\x9b', 156: '\\x9c', 157: '\\x9d', 158: '\\x9e', 159: '\\x9f', 160: '\\xa0', 161: '¡', 162: '¢', 163: '£', 164: '¤', 165: '¥', 166: '¦', 167: '§', 168: '¨', 169: '©', 170: 'ª', 171: '«', 172: '¬', 173: '\\xad', 174: '®', 175: '¯', 176: '°', 177: '±', 178: '²', 179: '³', 180: '´', 181: 'µ', 182: '¶', 183: '·', 184: '¸', 185: '¹', 186: 'º', 187: '»', 188: '¼', 189: '½', 190: '¾', 191: '¿', 192: 'À', 193: 'Á', 194: 'Â', 195: 'Ã', 196: 'Ä', 197: 'Å', 198: 'Æ', 199: 'Ç', 200: 'È', 201: 'É', 202: 'Ê', 203: 'Ë', 204: 'Ì', 205: 'Í', 206: 'Î', 207: 'Ï', 208: 'Ð', 209: 'Ñ', 210: 'Ò', 211: 'Ó', 212: 'Ô', 213: 'Õ', 214: 'Ö', 215: '×', 216: 'Ø', 217: 'Ù', 218: 'Ú', 219: 'Û', 220: 'Ü', 221: 'Ý', 222: 'Þ', 223: 'ß', 224: 'à', 225: 'á', 226: 'â', 227: 'ã', 228: 'ä', 229: 'å', 230: 'æ', 231: 'ç', 232: 'è', 233: 'é', 234: 'ê', 235: 'ë', 236: 'ì', 237: 'í', 238: 'î', 239: 'ï', 240: 'ð', 241: 'ñ', 242: 'ò', 243: 'ó', 244: 'ô', 245: 'õ', 246: 'ö', 247: '÷', 248: 'ø', 249: 'ù', 250: 'ú', 251: 'û', 252: 'ü', 253: 'ý', 254: 'þ', 255: 'ÿ', 256: 'Ġ', 257: '—', 258: '<|endoftext|>', 259: 'eĠ', 260: 'th', 261: 'dĠ', 262: 'tĠ', 263: 'Ġth', 264: 'in', 265: 'Ġa', 266: 'sĠ', 267: 'er', 268: 'yĠ', 269: 'on', 270: 'en', 271: 'ĠtheĠ', 272: 'edĠ', 273: 'of', 274: 'or', 275: ',Ġ', 276: '.Ġ', 277: 're', 278: 'IĠ', 279: 'ou', 280: 'Ġm', 281: 'ha', 282: 'to', 283: 'ndĠ', 284: 'es', 285: 'st', 286: 'll', 287: 'ing', 288: 'ar', 289: 'it', 290: 'an', 291: 'wa', 292: ',Ġa', 293: 'at', 294: 'Ġs', 295: 'atĠ', 296: 'ion', 297: 'theĠ', 298: 'no', 299: 'om', 300: 'for', 301: 'ch', 302: 'ofĠtheĠ', 303: 'ofĠ', 304: 'ĠmyĠ', 305: 'wh', 306: 'el', 307: 'ea', 308: 'ir', 309: 'ed', 310: 'toĠ', 311: 'hadĠ', 312: 'wi', 313: ',ĠandĠ', 314: 'Th', 315: 'ent', 316: 'is', 317: 'em', 318: 'up', 319: 'ingĠ', 320: 'itĠ', 321: 'al', 322: 'inĠ', 323: 'be', 324: 'gh', 325: 'ĠaĠ', 326: 'ereĠ', 327: 'ra', 328: 'os', 329: 'ver', 330: 'un', 331: 'with', 332: '.ĠTh', 333: 'ur', 334: 'lyĠ', 335: '.ĠIĠ', 336: 'enĠ', 337: 'ldĠ', 338: 'myĠ', 339: 'ter', 340: 'ri', 341: 'ow', 342: 'wasĠ', 343: 'ce', 344: 'res', 345: 'le', 346: 'im', 347: 'per', 348: 'whi', 349: 'veĠ', 350: 'ct', 351: 'ul', 352: 'ĠthatĠ', 353: 'upon', 354: 'us', 355: 'utĠ', 356: 'ca', 357: 'con', 358: 'ro', 359: 'llĠ', 360: ',ĠIĠ', 361: 'chĠ', 362: 'meĠ', 363: 'la', 364: 'ouldĠ', 365: 'ear', 366: 'um', 367: 'rea', 368: '.ĠI', 369: 'dea', 370: 'me', 371: 'fr', 372: 'sa', 373: '\\n\\n', 374: 'itsĠ', 375: 'ceĠ', 376: 'ionĠ', 377: 'ough', 378: 'ly', 379: 'ĠandĠ', 380: 'se', 381: 'notĠ', 382: 'orĠ', 383: 'from', 384: 'isĠ', 385: 'li', 386: 'andĠ', 387: 'entĠ', 388: '.\\n\\n', 389: 'leĠ', 390: 'ag', 391: 'ess', 392: 'lo', 393: 'di', 394: 'butĠ', 395: '!Ġ', 396: 'nĠ', 397: 'Ġp', 398: 'des', 399: 'thatĠ', 400: '.ĠTheĠ', 401: 'ain', 402: 'ĠatĠ', 403: 'qu', 404: 'de', 405: ';Ġ', 406: 'vi', 407: 'whichĠ', 408: '.ĠA', 409: 'Ġf', 410: 'ge', 411: 'erĠ', 412: 'haveĠ', 413: 'alĠ', 414: 'ex', 415: 'su', 416: 'ck', 417: 'ong', 418: 'esĠ', 419: 'hatĠ', 420: 'fe', 421: 'edĠtheĠ', 422: 'ep', 423: 'si', 424: 'stĠ', 425: 'sh', 426: 'sp', 427: 'fi', 428: 'essĠ', 429: 'str', 430: 'beenĠ', 431: 'len', 432: 'oundĠ', 433: 'was', 434: 'ol', 435: 'ab', 436: 'we', 437: 'inĠtheĠ', 438: 'ghtĠ', 439: 'omeĠ', 440: 'gl', 441: 'toĠtheĠ', 442: 'thĠ', 443: 'ut', 444: 'ci', 445: 'fir', 446: 'ever', 447: 'beĠ', 448: 'asĠ', 449: 'war', 450: 'couldĠ', 451: 'ofĠmyĠ', 452: 'aĠ', 453: 'fa', 454: '.ĠItĠ', 455: 'fĠ', 456: 'od', 457: 'whatĠ', 458: 'sti', 459: 'hor', 460: 'eyĠ', 461: 'oĠ', 462: 'ĠthereĠ', 463: 'ous', 464: 'pa', 465: 'now', 466: 'long', 467: 'leng', 468: 'tedĠ', 469: 'toĠmyĠ', 470: 'saw', 471: 'pp', 472: 'wall', 473: 'over', 474: 'rem', 475: 'il', 476: 'cou', 477: 'Ġar', 478: 'end', 479: 'ey', 480: 'wereĠ', 481: 'withĠ', 482: 'anĠ', 483: 'ation', 484: 'IĠhadĠ', 485: 'fromĠtheĠ', 486: 'anyĠ', 487: 'ĠthisĠ', 488: 'nd', 489: 'edĠmyĠ', 490: ',ĠtheĠ', 491: 'seem', 492: 'oneĠ', 493: 'dd', 494: 'and', 495: 'oreĠ', 496: 'cameĠ', 497: 'ofĠaĠ', 498: 'form', 499: 'though', 500: 'geĠ', 501: 'ma', 502: 'op', 503: 'ob', 504: 'ctĠ', 505: 'min', 506: 'ss', 507: 'y,Ġ', 508: 'nessĠ', 509: 'erv', 510: 'ofĠs', 511: 'edĠtoĠ', 512: 'hadĠbeenĠ', 513: 'byĠ', 514: 'ofĠa', 515: 'et', 516: 'eir', 517: 'pres', 518: 'ness', 519: 'ousĠ', 520: 'ti', 521: 'wouldĠ', 522: 'ĠasĠ', 523: 'hea', 524: 'ish', 525: 'kĠ', 526: 'wo', 527: 'pr', 528: 'mi', 529: 'ef', 530: 'onĠ', 531: 'own', 532: 'sel', 533: 'my', 534: 'ationĠ', 535: 'etĠ', 536: 'mo', 537: 'ort', 538: 'e,Ġ', 539: 'oseĠ', 540: 'too', 541: 'horr', 542: 'uponĠtheĠ', 543: 'ta', 544: 'cha', 545: 'keĠ', 546: 'desc', 547: 'seĠ', 548: '?Ġ', 549: 'va', 550: 'elyĠ', 551: 'uponĠmyĠ', 552: 'cl', 553: 'eltĠ', 554: 'withĠtheĠ', 555: ';ĠbutĠ', 556: 'sĠofĠtheĠ', 557: 'ĠmeĠ', 558: 'ityĠ', 559: 'ure', 560: 'br', 561: 'ect', 562: 'ustĠ', 563: 'outĠ', 564: 'stru', 565: 'which', 566: 'da', 567: 'pl', 568: 'dist', 569: 'seemedĠ', 570: 'ateĠ', 571: 'hear', 572: 'bleĠ', 573: 'eseĠ', 574: 's—', 575: 'ev', 576: 'Ġtheir', 577: 'sĠofĠ', 578: 'ity', 579: 'der', 580: 'Ġan', 581: 'att', 582: 'ofĠtheĠs', 583: '.ĠW', 584: 'ĠaĠs', 585: 'ofĠm', 586: 'cond', 587: 'pro', 588: 'ed,Ġ', 589: 'com', 590: 'flo', 591: 'ful', 592: 'ba', 593: 'gre', 594: 'edĠme', 595: 'forĠ', 596: 'dun', 597: 'dunge', 598: 'cir', 599: 'circ', 600: 'pris', 601: 'Ġag', 602: 'toĠs', 603: 'oi', 604: 'how', 605: 'e—', 606: 'wor', 607: 'emp', 608: 'stillĠ', 609: 'lesĠ', 610: 'horror', 611: 'par', 612: 'firstĠ', 613: 'ostĠ', 614: 'noĠ', 615: 'length', 616: 'if', 617: 'intoĠ', 618: 'estĠ', 619: 'd,Ġ', 620: 'ButĠ', 621: 'dden', 622: 'edĠmeĠ', 623: 'coun', 624: 'all', 625: 'bl', 626: 'ce,Ġ', 627: 'es,Ġ', 628: 'fo', 629: 'ĠmanyĠ', 630: 'clos', 631: 'therĠ', 632: 'ext', 633: 'hop', 634: 'deathĠ', 635: ',ĠandĠIĠ', 636: 'feltĠ', 637: 'ac', 638: 'quis', 639: 'quisit', 640: 'ori', 641: 'oul', 642: 'peri', 643: 'Ġapp', 644: 'press', 645: 'ps', 646: '.ĠAtĠ', 647: 'fra', 648: 'longĠ', 649: 'befor', 650: 'weĠ', 651: 'brea', 652: 'ingĠtheĠ', 653: 'strug', 654: 'veryĠ', 655: 'Ġal', 656: 'heart', 657: 'Ġthr', 658: 'layĠ', 659: 'son', 660: 'tim', 661: 'ction', 662: 'dungeon', 663: 'well', 664: 'ed—', 665: 'lef', 666: 'our', 667: 'prison', 668: 'edĠandĠ', 669: 'sĠtheĠ', 670: 'selfĠ', 671: 'ĠanĠ', 672: 'pend', 673: 'lengthĠ', 674: 'lea', 675: 'drea', 676: 'death', 677: 'cesĠ', 678: 'edĠinĠ', 679: 'idea', 680: 'ĠofĠ', 681: 'onlyĠ', 682: 'forĠaĠ', 683: 'withĠaĠ', 684: 'ceed', 685: 'oment', 686: ',ĠthereĠ', 687: 'everyĠ', 688: 'inĠmyĠ', 689: ',ĠasĠ', 690: 'inĠa', 691: 'rus', 692: 'rush', 693: 'ber', 694: 'In', 695: 'been', 696: 'med', 697: 'ret', 698: ',ĠatĠ', 699: 'vel', 700: 'co', 701: 'ew', 702: '.\\n\\nA', 703: 'reg', 704: 'ĠsomeĠ', 705: 'whenĠ', 706: 'lat', 707: 'Ġac', 708: 'Ġmo', 709: 'out', 710: 'back', 711: 'uchĠ', 712: 'IĠcouldĠ', 713: '.ĠM', 714: 'pos', 715: 'step', 716: 'pe', 717: 'someĠ', 718: 'mightĠ', 719: 'wardĠ', 720: 'antĠ', 721: '.ĠF', 722: 'nowĠ', 723: 'sur', 724: 'pendul', 725: 'pendulum', 726: '.ĠButĠ', 727: 'band', 728: 'when', 729: 'wasĠtheĠ', 730: 'distin', 731: 'fter', 732: 'voi', 733: 'soul', 734: 'hap', 735: 'fromĠ', 736: 'anc', 737: 'bur', 738: 'eel', 739: 'bri', 740: 'ud', 741: 'uponĠ', 742: 'ens', 743: '.ĠIĠsaw', 744: 'ment', 745: '.ĠAndĠ', 746: 'vis', 747: 'spir', 748: 'whileĠ', 749: 'not', 750: 'thoughtĠ', 751: 'thereĠ', 752: 'call', 753: '.ĠThen', 754: 'fin', 755: '.ĠIn', 756: 's,Ġ', 757: 'so', 758: 'tw', 759: ',ĠthatĠ', 760: 'ph', 761: 'inter', 762: 'eĠwh', 763: 'notĠh', 764: 'fre', 765: 'fulĠ', 766: 'endea', 767: 'endeav', 768: 'ther', 769: ',Ġin', 770: 'down', 771: 'ĠofĠtheĠ', 772: '.ĠTheyĠ', 773: 'tion', 774: 'less', 775: 'Ġthrough', 776: 'lim', 777: 'ingĠmyĠ', 778: 'pre', 779: 'aĠs', 780: 'effor', 781: ',ĠIĠhadĠ', 782: 'ainĠ', 783: 'into', 784: 'deĠ', 785: 'ĠatĠtheĠ', 786: 'ĠtheseĠ', 787: 'Ġawa', 788: 'floor', 789: 'by', 790: 'edĠfor', 791: 'ght', 792: 'ward', 793: 'rat', 794: 'them', 795: 'leftĠ', 796: 'hour', 797: 'iedĠ', 798: 'ard', 799: 'ung', 800: 'pit', 801: 'serv', 802: 'hal', 803: ';ĠandĠ', 804: 'Ġme', 805: 'hum', 806: 'ution', 807: 'sĠa', 808: 'entlyĠ', 809: 'ore', 810: '.ĠY', 811: '!ĠIĠ', 812: 'bla', 813: 'edĠto', 814: 'ĠtheirĠ', 815: 'cont', 816: 'del', 817: 'ic', 818: 'swe', 819: 'beforeĠ', 820: 'fig', 821: 'thing', 822: 'dar', 823: 'sup', 824: 'still', 825: 'ni', 826: 'iver', 827: 'woon', 828: 'cons', 829: 'attemp', 830: 'mostĠ', 831: 'Ġse', 832: 'al,Ġ', 833: 'sten', 834: 'ĠtheyĠ', 835: 'low', 836: 'orsĠ', 837: 'glesĠ', 838: 'whenĠIĠ', 839: 'cess', 840: 'fer', 841: 'atĠtheĠ', 842: 'descent', 843: 'ueĠ', 844: 'Ġthough', 845: 'tr', 846: 'sib', 847: 'eyes', 848: 'thatĠIĠ', 849: 'hand', 850: 'edĠitĠ', 851: 'minut', 852: 'ĠaroundĠ', 853: 'fear', 854: '.ĠMyĠ', 855: 'Ġthen', 856: 'pass', 857: 'struggl', 858: 'breath', 859: 'ition', 860: 'TheĠ', 861: 'kn', 862: 'onĠtheĠ', 863: 'onceĠ', 864: 'lightĠ', 865: 'Ġsu', 866: 'odĠ', 867: 's.ĠIĠ', 868: 'wallsĠ', 869: 'ig', 870: 'eyesĠ', 871: 'ran', 872: 'ĠmoreĠ', 873: 'terĠ', 874: 'dou', 875: 'oc', 876: 'tain', 877: ',Ġhow', 878: 'sc', 879: 'circu', 880: 'ledĠ', 881: 'itĠwasĠ', 882: 'chan', 883: 'rap', 884: 'mentĠ', 885: 'riv', 886: 'ly—', 887: 'lin', 888: '.ĠFor', 889: 'atedĠ', 890: 'myselfĠ', 891: 'nerv', 892: 'ureĠ', 893: 'formedĠ', 894: 'iron', 895: 'observ', 896: 'ven', 897: 'steel', 898: 'oh', 899: 'ith', 900: 'bos', 901: 'bosom', 902: 'bandag', 903: '—theĠ', 904: 'enceĠ', 905: 'cent', 906: 'reach', 907: 'quisitori', 908: '.ĠThisĠ', 909: 'period', 910: 'Ġthan', 911: 'whichĠIĠ', 912: 'evenĠ', 913: 'mov', 914: 'dead', 915: 'shu', 916: 'shudd', 917: 'shudder', 918: 'near', 919: 'theyĠ', 920: 'spe', 921: 'mean', 922: 'head', 923: 'restĠ', 924: 'mustĠ', 925: 'e.ĠTheĠ', 926: 'fu', 927: 'justĠ', 928: 'nessĠofĠ', 929: 'Ġma', 930: 'yetĠ', 931: 'first', 932: 'enseĠ', 933: 'pression', 934: 'ishĠ', 935: 'interv', 936: 'hasĠ', 937: 'ind', 938: 'wil', 939: 'rest', 940: 'Ġat', 941: 'haveĠbeenĠ', 942: 'uredĠ', 943: 'sure', 944: 'itionĠ', 945: 'distinct', 946: 'hi', 947: 'nat', 948: 'bus', 949: 'eryĠ', 950: 'sudden', 951: 'ound', 952: 'ult', 953: 'ofĠitsĠ', 954: 'ĠtheĠm', 955: 't—', 956: 'tri', 957: 'ofĠall', 958: 'ome', 959: 'har', 960: 'ffer', 961: 'imag', 962: 'anceĠ', 963: 'obj', 964: 'ble', 965: ',ĠbutĠ', 966: 'grew', 967: 'dĠtheĠ', 968: 'oin', 969: 'didĠ', 970: 'knew', 971: 'ofĠtheseĠ', 972: 'pla', 973: 'nedĠ', 974: 'cell', 975: 'moreĠ', 976: 'Up', 977: 'star', 978: 'dire', 979: 'ed,ĠandĠ', 980: 'vid', 981: 'doub', 982: 'wasĠaĠ', 983: 'y—', 984: 'ertain', 985: 'iv', 986: 'eĠofĠtheĠ', 987: '.ĠIĠhadĠ', 988: 'dis', 989: '.ĠInĠ', 990: 'countedĠ', 991: 'fell', 992: 'ĠtoĠtheĠ', 993: 'ty', 994: 'two', 995: 'itt', 996: 'tyĠ', 997: 'ros', 998: 'forĠtheĠ', 999: 'fall'}\n",
      "1000\n",
      "741\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the BPE tokenizer with a vocab size of 1000\n",
    "tokenizer = BPETokenizerImproved()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})\n",
    "print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))\n",
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aabd7ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 111, 409, 288, 44, 256, 73, 256, 281, 100]\n"
     ]
    }
   ],
   "source": [
    "# Test of encoding\n",
    "input_text = \"So far, I had\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81222150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 111, 409, 288, 44, 256, 73, 256, 281, 100, 258]\n"
     ]
    }
   ],
   "source": [
    "# Test of encoding with allowed_special\n",
    "input_text = \"So far, I had<|endoftext|>\"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f092ca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 26\n",
      "Number of token IDs: 11\n"
     ]
    }
   ],
   "source": [
    "# Print length information\n",
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a2e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 111, 409, 288, 44, 256, 73, 256, 281, 100, 258]\n",
      "So far, I had<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Test of decoding\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba016a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 -> S\n",
      "111 -> o\n",
      "409 ->  f\n",
      "288 -> ar\n",
      "44 -> ,\n",
      "256 ->  \n",
      "73 -> I\n",
      "256 ->  \n",
      "281 -> ha\n",
      "100 -> d\n",
      "258 -> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Better understanding of decoding\n",
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35832da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone with \\n newline characters!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same text reproducible\n",
    "tokenizer.decode(tokenizer.encode(\"Hello everyone with \\n newline characters!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ef3fb",
   "metadata": {},
   "source": [
    "## Saving and loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a4b99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained tokenizer\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bad5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer2 = BPETokenizerImproved()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "068b75f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, I had<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Same results as before\n",
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b97fb20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone with \\n newline characters!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same text reproducible\n",
    "tokenizer.decode(tokenizer.encode(\"Hello everyone with \\n newline characters!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f7b1b",
   "metadata": {},
   "source": [
    "## Load the original GPT-2 BPE tokenizer from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30df588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for downloading file\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file_if_absent(url, filename, search_dirs):\n",
    "    for directory in search_dirs:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"{filename} already exists in {file_path}\")\n",
    "            return file_path\n",
    "\n",
    "    target_path = os.path.join(search_dirs[0], filename)\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with open(target_path, \"wb\") as out_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    out_file.write(chunk)\n",
    "        print(f\"Downloaded {filename} to {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}. Error: {e}\")\n",
    "\n",
    "    return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d7f1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded vocab.bpe to ./vocab.bpe\n",
      "Downloaded encoder.json to ./encoder.json\n"
     ]
    }
   ],
   "source": [
    "# Download files if not already present in this directory\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "paths = {}\n",
    "for url, filename in files_to_download.items():\n",
    "    paths[filename] = download_file_if_absent(url, filename, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 files\n",
    "tokenizer_gpt2 = BPETokenizerImproved()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(vocab_path=paths[\"encoder.json\"], bpe_merges_path=paths[\"vocab.bpe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79fce15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "# Check the length\n",
    "print(len(tokenizer_gpt2.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b45cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2396, 1290, 11, 314, 550]\n"
     ]
    }
   ],
   "source": [
    "# Use the GPT-2 tokenizer\n",
    "input_text = \"So far, I had\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35541680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, I had\n"
     ]
    }
   ],
   "source": [
    "# Test decode with GPT-2 tokenizer\n",
    "print(tokenizer_gpt2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8f2b266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2396, 1290, 11, 314, 550]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for correctness\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
