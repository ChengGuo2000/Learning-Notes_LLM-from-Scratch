{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cc53c7",
   "metadata": {},
   "source": [
    "# Appendix E\n",
    "I am using the UCI Sentiment Labelled Sentences dataset instead of the UCI SMS Spam Collection dataset to get some novel results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff771e",
   "metadata": {},
   "source": [
    "# E.2 - Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a47774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "import pandas as pd\n",
    "from previous_chapters import random_split\n",
    "data = []\n",
    "with open(\"../chapter-6-learn/sentiment_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        text, label = line.rsplit('\\t', 1)\n",
    "        data.append((text, label))\n",
    "balanced_df = pd.DataFrame(data, columns=[\"Text\", \"Label\"])\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"../chapter-6-learn/train.csv\", index = None)\n",
    "validation_df.to_csv(\"../chapter-6-learn/validation.csv\", index = None)\n",
    "test_df.to_csv(\"../chapter-6-learn/test.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4448336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating PyTorch datasets\n",
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"../chapter-6-learn/train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"../chapter-6-learn/validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"../chapter-6-learn/test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a4dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PyTorch data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True,\n",
    "                          num_workers = num_workers, drop_last = True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size,\n",
    "                        num_workers = num_workers, drop_last = False)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size,\n",
    "                        num_workers = num_workers, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65263cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 103])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Print the batch dimensions\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533fe72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 training batches\n",
      "38 validation batches\n",
      "75 test batches\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a08dfe",
   "metadata": {},
   "source": [
    "# E.3 - Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fefa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 10:30:37.530516: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Loading a pretrained GPT model\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"So far, I had\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size = model_size, models_dir = \"../chapter-5-learn/gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b88dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, I had no idea what to expect.\n",
      "\n",
      "I was so excited to see the\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model generates coherent text\n",
    "from previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text\n",
    "text_1 = \"So far, I had\"\n",
    "token_ids = generate_text_simple(model = model, idx = text_to_token_ids(text_1, tokenizer), \n",
    "                                 max_new_tokens = 15, context_size = BASE_CONFIG[\"context_length\"])\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834997d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for classification fine-tuning\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features = 768, out_features = num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63716271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 50.00%\n",
      "Validation accuracy: 48.75%\n",
      "Test accuracy: 52.50%\n"
     ]
    }
   ],
   "source": [
    "# Calculate initial classification accuracy\n",
    "from previous_chapters import calc_accuracy_loader\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches = 10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches = 10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches = 10)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b927cd",
   "metadata": {},
   "source": [
    "# E.4 - Parameter-efficient fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d667900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a LoRA layer\n",
    "import math\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a = math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4346ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing a LinearWithLoRA layer with Linear layers\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb788e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply LoRA to the earlier defined GPTModel\n",
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca79367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "# Freeze the original model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160d00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "# Reaplce the linear layers\n",
    "replace_linear_with_lora(model, rank = 16, alpha = 16)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11fad7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Verify that the layers have been modified as intended\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3acecbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 50.00%\n",
      "Validation accuracy: 48.75%\n",
      "Test accuracy: 52.50%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the initial classification accuracy\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches = 10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches = 10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches = 10)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "652d599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.587, Val loss 2.977\n",
      "Ep 1 (Step 000050): Train loss 0.738, Val loss 0.714\n",
      "Ep 1 (Step 000100): Train loss 0.705, Val loss 0.712\n",
      "Ep 1 (Step 000150): Train loss 0.689, Val loss 0.692\n",
      "Ep 1 (Step 000200): Train loss 0.694, Val loss 0.699\n",
      "Ep 1 (Step 000250): Train loss 0.694, Val loss 0.691\n",
      "Training accuracy: 45.00% | Validation accuracy: 52.50%\n",
      "Ep 2 (Step 000300): Train loss 0.707, Val loss 0.717\n",
      "Ep 2 (Step 000350): Train loss 0.688, Val loss 0.695\n",
      "Ep 2 (Step 000400): Train loss 0.688, Val loss 0.694\n",
      "Ep 2 (Step 000450): Train loss 0.695, Val loss 0.696\n",
      "Ep 2 (Step 000500): Train loss 0.736, Val loss 0.671\n",
      "Training accuracy: 42.50% | Validation accuracy: 55.00%\n",
      "Ep 3 (Step 000550): Train loss 1.210, Val loss 0.991\n",
      "Ep 3 (Step 000600): Train loss 0.925, Val loss 0.798\n",
      "Ep 3 (Step 000650): Train loss 0.682, Val loss 0.733\n",
      "Ep 3 (Step 000700): Train loss 0.674, Val loss 0.708\n",
      "Ep 3 (Step 000750): Train loss 0.715, Val loss 0.695\n",
      "Training accuracy: 52.50% | Validation accuracy: 47.50%\n",
      "Ep 4 (Step 000800): Train loss 0.689, Val loss 0.695\n",
      "Ep 4 (Step 000850): Train loss 0.688, Val loss 0.694\n",
      "Ep 4 (Step 000900): Train loss 0.679, Val loss 0.708\n",
      "Ep 4 (Step 000950): Train loss 0.626, Val loss 0.689\n",
      "Ep 4 (Step 001000): Train loss 0.504, Val loss 0.650\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 5 (Step 001050): Train loss 0.530, Val loss 0.767\n",
      "Ep 5 (Step 001100): Train loss 0.651, Val loss 0.781\n",
      "Ep 5 (Step 001150): Train loss 0.472, Val loss 0.582\n",
      "Ep 5 (Step 001200): Train loss 0.500, Val loss 0.743\n",
      "Ep 5 (Step 001250): Train loss 0.535, Val loss 0.544\n",
      "Ep 5 (Step 001300): Train loss 0.488, Val loss 0.575\n",
      "Training accuracy: 72.50% | Validation accuracy: 82.50%\n",
      "Training completed in 179.71 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning a model with LoRA layers\n",
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dadaea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAEiCAYAAABTO2OcAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAASYtJREFUeJzt3Qd4U2XbB/B/mqR7L6BAgcqeIltAQGQoIqCIoiLgQBQURdSXV2Top7jFiSAq+gqyFFRkiMjee0/Zo3vvdb7rftKEtLTQmabt/8d1rswmJw8nuc8zb52maRqIiIioTDmU7csTERERAy4REZGNsIZLRERkAwy4RERENsCAS0REZAMMuERERDbAgEtERGQDDLhEREQ2wIBLRERkAwy4RFVQ9+7d8eKLL5b3bhBVKQy4RMUwYsQI6HS667a+ffuyPIkoX4b87yaim5Hg+v333+e6z8nJiQVHRPliDZeomCS4Vq9ePdfm4+OjHlu/fj0cHR2xadMmy/Pff/99BAYGIiwsTN1etWoVunTpAm9vb/j5+eHee+/Fv//+a3n+uXPnVK150aJF6Nq1K1xcXNCuXTucPHkSu3btQtu2beHu7o67774bERERuWrfAwcOxLRp0xAQEABPT0+MHj0a6enpBX6WtLQ0TJgwATVr1oSbmxs6dOigPoPZ+fPn0b9/f/X55PFmzZphxYoVBb7eV199hQYNGsDZ2RnVqlXD4MGDLY9lZ2dj+vTpqFevnvpMrVq1wpIlS3L9/eHDh9Xnks8nfz9s2DBERkbmahJ/4YUX8Oqrr8LX11eV/dSpUwv1/0ZUXhhwicqwj1QCRVxcHPbt24c33ngDc+bMUQFEJCUlYfz48di9ezfWrl0LBwcHDBo0SAUka1OmTMGkSZOwd+9eGAwGPPLIIyrQfPrppyqgnz59GpMnT871N/J6x44dU0Hz559/xq+//qoCcEHGjh2Lbdu2YcGCBTh48CAefPBBVYM/deqUenzMmDEqKG/cuBGHDh3Ce++9p4JhfuTzSDB88803ceLECXVicccdd1gel2D7448/4uuvv8aRI0fw0ksv4bHHHsOGDRvU47GxsbjzzjvRunVr9Vry93KSMmTIkFzv88MPP6jgv2PHDnUyI++3Zs2aIv9fEdmMpOcjoqIZPny4ptfrNTc3t1zb22+/bXlOWlqaduutt2pDhgzRmjZtqj399NM3fM2IiAhJlakdOnRI3T579qy6PWfOHMtzfv75Z3Xf2rVrLfdNnz5da9SoUa598/X11ZKSkiz3zZw5U3N3d9eysrLU7W7dumnjxo1T18+fP68+y+XLl3PtT8+ePbWJEyeq6y1atNCmTp1aqLL55ZdfNE9PTy0+Pv66x1JTUzVXV1dt69atue5/8skntaFDh6rrb731lta7d+9cj1+8eFF97hMnTlj2v0uXLrme065dO+21114r1D4SlQf24RIVU48ePTBz5sxc90nzppk0Kc+bNw8tW7ZEnTp18Mknn+R6rtQepWYqNTRpLjXXbC9cuIDmzZtbnid/b2auHbdo0SLXfeHh4bleW5ppXV1dLbc7deqExMREXLx4Ue2LNamxZmVloWHDhrnulxqtNHULqbE+++yz+Ouvv3DXXXfhgQceyLVf1nr16qXeIyQkRNWSZZOau+yP1MaTk5PVc6xJc7fUaMWBAwewbt26fGvQ0uRu3s+871+jRo3ryoHInjDgEhWTNGfWr1//hs/ZunWruoyOjlab/I2Z9IlKYPrmm28QFBSkAq4E2rx9rUaj0XJd+nTzuy9vM3RRSCDW6/XYs2ePurRmDnpPPfUU+vTpgz///FMFXWkW/uijj/D8889f93oeHh6q+Vuas+W5clIh/avS7yzvJeR1pL84vwFn8hwpG2m2zkuCan7lUhrlQFTWGHCJyojUxqR/UgLqwoULMXz4cPz999+qrzYqKkr1b8pjMiBKbN68udTeW2qJKSkpalCS2L59uwqetWvXvu65UrOUGq7UDs37kh/5Wxl8JdvEiRPVvucXcIX0NUtNWDbpg5aBYf/884+q2UpglVp8t27d8v3b2267Db/88gvq1q2rXoeosuDRTFRM0uQaGhqa+wtlMMDf318FMBkIJLXCkSNHqmZVaQaWWuErr7yiRvtKc+3s2bNVrU0C0H/+859S+7+QWvKTTz6pBlvJaGcJejIwSoJ9XtJE++ijj+Lxxx9X+ycBWEY9y8Arabbt16+fGgAmo4bluTExMarJt0mTJvm+9/Lly3HmzBk1UEo+p4xmlppno0aNVO1XRkPLiYjcJ6O0ZVDZli1b1GhqOSmRAVoSzIcOHWoZhSxN0TKgSwad5a2FE1UUDLhExSSjZ62bOIUElePHj+Ptt99WU2kk+Ah5ngRXCSK9e/dWfawSQKRvVJqR5e8+++wzNbq5NPTs2VNNy5GgJycG8r43mjYj84n/7//+Dy+//DIuX76sTho6duyopioJOYGQQHjp0iUVGOUEIm+ftJnUZmVUtLxfamqq2g8ZKS1TicRbb72lpitJs7QEZnm+1Gr/+9//qseleV0C8GuvvabKSvZfmt7lPfM7YSCqKHQycqq8d4KISo/Mw5WpNcuWLWOxEtkRni4SERHZAAMuERGRDbBJmYiIyAZYwyUiIrIBBlwiIiIbYMAlIiKyAQbcHF9++aVa2UbSiUlqsp07d6KykXmPkt5NFh+QNHGSwk1WO7Im8yZlvqUsyiArE8maueZ0cmaySIMshiBr48rryEIOmZmZuZ4jy/rJ3EpZVUiWP5w7dy4qonfffVctGSgLP5ixjKDm6srCHnKcyGpWsqiHZPYxk9mGsqSjzD+Wx2XFKXPmITNZ6lIW3JB5vTIXVxbqMC/9aCaZi2T1K/leykpXkhXI3smcZckMZU4/eMstt6i5x9YzMKta+UiWqf79+6s51vJ9yjtlzZblsXjxYjRu3Fg9R47bG6WZLHXlkjLBzixYsEBzdHTUvvvuO+3IkSMqq4u3t7cWFhamVSZ9+vTRvv/+e+3w4cPa/v37tXvuuUcLDg7WEhMTLc8ZPXq0Vrt2bZWNZvfu3VrHjh2122+/3fJ4Zmam1rx5c+2uu+7S9u3bp61YsULz9/e3ZJURZ86cURlhxo8frx09elT7/PPPVTaaVatWaRXJzp07tbp162otW7a0ZNYRVb2MoqOjtTp16mgjRozQduzYoT7L6tWrtdOnT1ue8+6772peXl7asmXLtAMHDmj33XefVq9ePS0lJcXynL59+2qtWrXStm/frm3atEmrX7++JWOQiIuL06pVq6Y9+uij6piVTEkuLi7arFmzNHsmGaP8/Py05cuXq4xPixcvVpmaPv300ypbPvIdeP3117Vff/1VZX1aunRprsdtVR5btmxR37P3339ffe8mTZqkGY1GS4aussaAq2la+/bttTFjxlgKRVKYBQUFqbRnlVl4eLg6+Dds2KBux8bGqoNPfiDMjh07pp6zbds2yxfHwcFBCw0NzZX6TdKxSTo68eqrr2rNmjXL9V4PPfSQCvgVRUJCgtagQQNtzZo1uVLZsYw0lQIvb2o8a9nZ2Vr16tW1Dz74wHKflJuTk5P6ERTyYyfH1a5duyzPWblypabT6SxpAr/66ivNx8fHclyZ39s6FaE96tevn/bEE0/kuu/+++9XgUBU9fJBnoBry/KQVJny/2OtQ4cO2jPPPKPZQpVvUpY1ZyVLijRhmMnycXJbEnJXZrKGrXVKOSmHjIyMXGUhTS/BwcGWspBLaYYxp4kTsl5wfHy8SiZufo71a5ifU5HKU5rVpdk87+dgGQG///472rZtq5LUS5eCrL0sax+bnT17Vq0xbV12Xl5eqqvG+jiSZkF5HTN5vnz3JF2h+TmyNKWkObQ+jqQbRNZztle33367Wof65MmTlkQSkphC1qIWVb188rJleZT3b1OVD7iSh1T6XKwDiJDbeRemr0xk4Xjpl+zcubMl96p8XjlY5cAuqCzkMr+yMj92o+dIUJYMNvZO1jiW9HLS550Xywhq/WPJAyxrJK9evVrlyZU1oX/44QdLGYkbfafkUoJ13sQPcvJXlGPNHkkSiocfflidrEoKQTkhke+a9D+Kql4+edmyPAp6jq3Ki8kLqiipwR0+fLhUU8JVBpKgfdy4cVizZo0aVEH5n6xJTeOdd95RtyWgyLH09ddfq2w/Vd2iRYswb948zJ8/XyVs2L9/vwq4MmCI5VO1VfkarmRFkXRfeUfiyu3q1aujMpI0bZLFRlKs1apVy3K/fF5pYpeF7wsqC7nMr6zMj93oOTK60Jyf1V5Jk7HkhZUR1nIGLduGDRtUJh+5LmfDVb2MZCRp06ZNc90nqfpk9Lr1Z7zRd0oupZytyUh3GYlalHK0RzJq31zLle6XYcOGqXSE5haTql4+edmyPAp6jq3Kq8oHXGlCbdOmjepzsT6Dl9udOnVCZSLjFSTYLl26VCUDl2kL1qQcpAnMuiyk/0N+SM1lIZeHDh3KdfBLbVAChflHWJ5j/Rrm51SE8pS0dvL5pFZi3qQ2J82B5utVvYykGyLvdDLpr5QUekKOK/kBs/580p0gfW3WZSQnLXKCYybHpHz3pO/O/ByZTiLjCqzLSFIZSp5de5WcnHxdGkE5qZfPJqp6+eRly/Io9++dTYZmVYBpQTIibu7cuWo03KhRo9S0IOuRuJXBs88+q4ber1+/Xrt69aplS05OzjXlRaYK/fPPP2rKS6dOndSWd8pL79691dQimcYSEBCQ75SXV155RY1y/vLLLyvMlJf8WI9SFlW9jGS6lMFgUNNfTp06pc2bN099lp9++inXNA/5Dv3222/awYMHtQEDBuQ7zaN169ZqatHmzZvVqHDraR4yUlWmeQwbNkxN85DvqbyPPU57sTZ8+HCtZs2almlBMhVGpoXJ6P2qWj4y6n/fvn1qk7Dz8ccfq+vnz5+3aXnItCA5dj/88EP1vZsyZQqnBZUHmQcpP6IyH1emCclcr8pGDvT8NpmbayYH+HPPPaeG18vBOmjQIBWUrZ07d067++671Rw3+SF5+eWXtYyMjFzPWbdunXbrrbeq8gwJCcn1HhU94LKMNO2PP/5QJxVyotq4cWNt9uzZucpMpnq88cYb6gdQntOzZ0/txIkTuZ4TFRWlfjBljqpMKxs5cqT6YbYmczJlCpK8hgQx+WG2d/Hx8ep4kd8TZ2dndfzLHFTr6SpVrXzk9wD5/PbIyYmty2PRokVaw4YN1W+TTF/8888/NVthtiAiIiIbqPJ9uERERLbAgEtERGQDDLhEREQ2wIBLRERkAwy4RERENsCAS0REZAMMuDnS0tIwdepUdUn5YxndGMvn5lhGLKOqfAxxHq7VUmKSEkpS1skSfHQ9ltGNsXxujmXEMqrKxxBruERERDbAgEtERGQDFTofrqRn2rdvn0qZljc7R1ElJCSoy8uXL6smC2IZ8RgqffyesYwq4zEkWYskzZ/khpY0npWyD3fXrl1o3759ee8GERERdu7ciXbt2lXOGq7UbM0fUpJiExER2drVq1dV5c8ckyplwDU3I0uwrVWrVnnvDhERVWEON+na5KApIiIiG2DAJSIisgEGXCIiIhuo0H24REQ3kpWVhYyMDBYSlYjRaIRery/ZizDgXrPnfAzC41PRpYE/PJyNJS5YIio/MtsxNDQUsbGx/G+gUuHt7Y3q1atDp9MV+zVYw80xdv5eXI1LxW9jOqNVbe/S+R8ionJhDraBgYFwdXUt0Y8kVW2apiE5ORnh4eHqdkmmoDLg5gjwcFIBNyKh4mWgIKLczcjmYOvn58eioRJzcXFRlxJ05bgqbvMyB03lCHB3UpcRiQy4RBWZuc9WarZEpcV8PJVkTAADbo5Az5yAyxouUaXAZmSyt+OJATdPDTc8IbXEhUpERJQXA65VH65gDZeIKou6detixowZhX7++vXrVU2urEd3z507V436rWoYcHMw4BJReZEgd6Nt6tSpxc6oNmrUqEI///bbb1cL8Xt5eRXr/ejGOEo5b8DloCkisjEJcmYLFy7E5MmTceLECct97u7uuaapyEjsG+VdNQsICCjSfjg6Oqq5plQ2WMPNEeDubGlSrsApgomoApIgZ96kdim1WvPt48ePw8PDAytXrkSbNm3g5OSEzZs3499//8WAAQNUSjgJyJKH9e+//75hk7K87pw5czBo0CA16rZBgwb4/fffC2xSNjf9rl69Gk2aNFHv07dv31wnCJmZmXjhhRfU82Qa1muvvYbhw4dj4MCBRSqDmTNn4pZbblFBv1GjRvjf//5neUx+k6WWHxwcrD5/UFCQek+zr776Sn0WZ2dnVR6DBw+GPWLAzVPDTc3IRkJaZnn+nxBRWSxekJ5p8600T97/85//4N1338WxY8fQsmVLJCYm4p577sHatWuxb98+FQj79++PCxcu3PB1pk2bhiFDhuDgwYPq7x999FFER0cX+HxZ9OHDDz9UAXDjxo3q9SdMmGB5/L333sO8efPw/fffY8uWLYiPj8eyZcuK9NmWLl2KcePG4eWXX8bhw4fxzDPPYOTIkVi3bp16/JdffsEnn3yCWbNm4dSpU+r1W7RooR7bvXu3Cr5vvvmmahVYtWoV7rjjDtgjNinncHHUw8PJoIKt1HI9ubwjUaWRkpGFppNX2/x9j77ZB66OpfMzKwGlV69eltu+vr5o1aqV5fZbb72lApfUWMeOHVvg64wYMQJDhw5V19955x189tln2LlzpwrY+ZF5p19//bWqfQp5bdkXs88//xwTJ05UtWbxxRdfYMWKFUX6bB9++KHar+eee07dHj9+PLZv367u79GjhwryUtu/66671LrGUtOVhO9CHnNzc8O9996rWgLq1KmD1q1bwx6xhmuFA6eIyF61bds2122p4UpNU5p6pTlXmnul9nuzGq7Ujs0kUHl6elqWLcyPND2bg615aUPz8+Pi4hAWFmYJfkJWYZKm76I4duwYOnfunOs+uS33iwcffBApKSkICQnB008/rU4spClbyEmIBFl5bNiwYaq2LbVye8QarhV/DyeciUzi1CCiSsbFqFe1zfJ439IiwdGaBNs1a9aoWmD9+vXV8oPSd5menn7D15EaojXps83Ozi7S8209zqV27dqquVj6qOUzS034gw8+wIYNG1Stdu/evar/+a+//lIDzqS/V0Zo29vUI9ZwrbCGS1Q5SZCQpl1bb2W52pX0l0ozrDTlSn+mNLmeO3cOtiQDvGSQkgQ3MxlBLQGwKJo0aaI+jzW53bRpU8ttOaGQPmppApfgum3bNhw6dEg9JiO2pbn5/fffV33TUg7//PMP7A1ruFYCOTWIiCoIGZX766+/qiAkgf2NN964YU21rDz//POYPn26qmU3btxY9enGxMQU6WTjlVdeUQO5pO9VAucff/yhPpt51LWMlpZA3qFDB9XE/dNPP6kALE3Jy5cvx5kzZ9RAKR8fH9V/LOUgI53tDQNuPjXc8HgmMCAi+/bxxx/jiSeeUItV+Pv7q+k4MkLY1uR9JR3i448/rvpvZaGNPn36FCmjzsCBA/Hpp5+q5nEZrVyvXj016rl79+7qcWkalhHaMphKAq/U6CUoyzQkeUyCszQjp6amqhORn3/+Gc2aNYO90WkVeNLppUuXVNv+xYsXUatWrRK/3uLdF/HKkoO4o2EAfnzi2iAAIqo45Ef37Nmz6kdb5mWSbUntUpqIpcYqI6erwnF1qZCxiDVcK+zDJSIqmvPnz6vBSt26dUNaWpqaFiSB6ZFHHmFR5sFBU1YYcImIisbBwUH1scpKVzKVRwYySd+r1HIpN9Zw8wm40UlpyMrWoHcouxGGRESVgTSl5h1hTPljDdeKn5sTJMZma0AUkxgQEVEpYsC1IjVaP0sieo5UJiKi0sOAm0dATsBlmj4iIipNDLh5cOAUERGVBQbcPBhwiYioLDDg5sGAS0REZYEBt6D1lDloiogqGFkK8cUXX7Tcrlu3LmbMmHHDv5E1j4uaML4sX+dGZPnGW2+9FRUVA24erOESka1JAoKCEsBv2rRJBTPJglNUksVH1ja2RdC7evUq7r777lJ9r8qmXAPuzJkzVTJkSYAsW6dOnbBy5cry3CWOUiYim3vyySdVnldZkzcvWcRfks9bJ44vrICAAJVdxxYkPaCTk6mFkOww4Moiz5IBYs+ePdi9ezfuvPNODBgwAEeOHCm3fWINl4hs7d5771XBUZZItJaYmIjFixergBwVFYWhQ4eiZs2aKohKxhzJinMjeZuUT506pdLYyeL7kmtWgnx+2X8aNmyo3iMkJESl/cvIyFCPyf5NmzYNBw4cULVu2cz7nLdJWZZ4lN90SaMnWX1GjRqlPo+Z5PKVLEGSIahGjRrqOWPGjLG8V2ETJbz55psqlkiwl5r3qlWrLI+np6dj7Nix6vXlM0s6P0klKCRvj9TWg4OD1d8GBQXhhRdeQKVd2lGaUay9/fbbqta7ffv2ckutZA64iWmZSE7PVEmkiaiSSE8q+t/onQB9zu9AViaQlQboHACjy41f19Gt0G8hCdQlvZ0Er9dff92SS1aCraSjk0ArwapNmzYqIEqL4J9//olhw4bhlltuQfv27QsVnO6//36VMH7Hjh2Ii4vL1d9r5uHhofZDApAEzaefflrd9+qrr+Khhx7C4cOHVVAz56qVJPR5JSUlqRR90mopzdrh4eF46qmnVPCzPqlYt26dCoZyefr0afX6EjTlPQtDUvp99NFHmDVrlsql+9133+G+++5TlTZJ0yfJ6n///XcsWrRIBVbJ5iOb+OWXX/DJJ59gwYIFKt5IikE5kShLdhNN5KCSg0v+o+Q/qby4OxngYtQjJSMLkQnpCPazmyIiopJ6J6jof/PgXKDZINP1438Ai0cAdboAI/+89pwZLYDkqNx/NzWuSG8juW0/+OADbNiwwZIHVpqTH3jgARXUZJswYUKuxO+rV69WwaQwAVcC5PHjx9XfSDAV77zzznX9rpMmTcpVQ5b3lKAkAVdqq+7u7uoEQZqQCzJ//nyVzu7HH3+Em5vpxOOLL75Qlaz33ntPBX0hCePlfsmdK8nr+/Xrh7Vr1xY64ErtWE5AHn74YXVbXluCt9Tqv/zyS1y4cEEF3i5duqiTGKnhmslj8hkk4b3RaFQBuTDlWKEHTckZlPwHSpV+9OjRWLp0qWrqyI+kfpIEy+YtISGh1PdH/lMsiegTUkv99YmI8iMBR5LJSy1NSI1PBkxJc7K5UiL5ZaUp2dfXV/1uSvCUwFEYx44dU4kGzMFW5Fe5Wbhwocr6I8FI3kMCcGHfw/q9WrVqZQm2onPnzqqWfeLECct9UrO0TlQvtV2pDReGxIArV66o17Umt+X9zc3W+/fvR6NGjVRzsaQRNHvwwQeRkpKims0lwEvsyczMRFkq9+qbFIQUiDRvLFmyBMOHD1dnePkFXWl7l/6DMpGdDSSFA06eKuBeiE7m1CCiyua/V4rXpGzWuL/pNaRJ2dqLh0pt8JTUXKV2JrVbaS6WPLNCar/ShCq1Nwm6EsykSVj6KUvLtm3b8Oijj6rfWWkSllq11G6l2bYsGI3G6yo8EpRLy2233aZy88pgXKnhDxkyRNVoJdbIyYcEf7lf+rKfe+45SwtD3v2qNDVcR0dH1K9fX/VNSECVsyI5qPIzceJEFZjN29GjR0tvR77tBXzUCDi7kSOViSor6Vct6mbuvxVyXe6z7r8t6HWLQQKC5JeVJllpjpVmZnN/rqTAk0Gljz32mPqdlJrZyZMnC/3akp9W+i9l+o6ZjJextnXrVtXsKv3IMjJammMlwXyuj+roqGrbN3sv6Q+VLkKzLVu2qM8mlazSIP3YUlvPmxpQbltX2OR50jf8zTffqNq79N1GR0erx6SJXJq5pa93/fr16oRDWl0rbQ03Lzm7kabj/Eizs/Wwc2lSKDUeOf0RcRcR4FFLXeXiF0RkS9KEK8FBKhfy+yZNomYS/KRmJkFR+j4//vhjhIWFFdgFl5fU7GT0sbQiSk1OXl8CqzV5D2k+llqtJJSXgVnS1GpN+nWl1igtkzI6WAZU5Z0OJLXkKVOmqPeSkcARERGq5i6DvMz9t6XhlVdeUe8jLQEy2EpaBWS/5s2bpx6XMpJmahlQJcFexglJU7m3t7cavCUnDh06dFAjsn/66ScVgK37eStVDVcOqo0bN+LcuXPqrEJuy1mG/GfZnFdtq4DL1aaIqHxIs3JMTIxq0rXub5W+VGkilftlUJUEDplWU1gScCR4Sr+lDA6SUcMyM8SajPB96aWX1GhiCWAS3GVakDUZxCWLdPTo0UNNZcpvapIEMOlflpqkBO7BgwejZ8+eaoBUaZJ+2fHjx+Pll19WzewyelpGJcuJg5CTgffff1/V1mU/JNasWLFClYUEXan1Sp+vzHGWpuU//vhDTU8qKzpNJiOV44ElI9KkiUP6CuRDy4izXr16FervZZK4tMNLM4mcaZXIti+B1f8Fmt2PBXWm4T+/HsKdjQPx3Yh2JXtdIrIpGR0rNbB69eqpuZdEZX1cFTYWlWuT8rfffgu74ZVTSHGXWMMlIqLKN2jKbrBJmYiIyhADbt6AmxCKAFfTqMDIxDRkZ5dbizsREVUiDLhmbv6AQdrlNfhlmVaMyczWEJtS+HU9iYiICsKAayZz3XL6cR0TL8PXzVFd59QgIiIqDQy4BQ2ccufyjkQVWWmuWESUXQrHk90tfGE/A6dCcCIsgTVcogpGVkKSeZayzq7ME5Xb5tWaiIpKZs7K8pmyeIccV3I8FRcDrjWOVCaq8ORHUeZKyvx+CbpEpUEW85CMQnJ8FRcDrjXvnBpu7EUE+HO1KaKKSmoh8uMo2V9utu4v0c1IRiNJSVjSlhIGXGv1ewFP/QN4ByNgryn1X0Ri/us6E5F9kx9HyfpSVplfiIqKAdeae4BpAxDoaUp5FR7PgEtERCXHUcoFMI9SZg2XiIhKA2u4ee37CQg7iqCQh9VNzsMlIqLSwICb1565wKVdCKjeVjLwIi4lA2mZWXAy6EulwImIqGpik3Jeze4HOo2Fa2AIjHrzmsqm/lwiIqLiYg03r07PqQsJtQHuUbgSl6qalWt6uxS7kImIiFjDvYEAT1OS4fD4VB4pRERUIqzh5iXrZSaFA8lRHKlMRESlhgE3r/AjwNddAFc/BNT/Td3FkcpERFRSbFIuKGNQchRquJqSzzPgEhFRSTHg5uXsDTh6qKt1DNHqkgGXiIhKigH3Bonoa+oi1WV4Apd3JCKikmHAzU9OwA3IjlCXrOESEVFJMeDeIE2fd3qoZT1lSUJMRERUXAy4N6jhuqeaAm56ZjbiUzOLXchEREQMuPnxClYX+vhL8HA2zZxiszIREZUEA+6NpgbFXUSAR06aPg6cIiKiEmDAvUEfLuIvo5q7qYYbnsDlHYmIqPgYcPPjXh3Q6YHsTNzikqzuYg2XiIhKggE3P3oD4FlTXQ0xxlhGKhMRERUXA+5N+nFrO0SpS9ZwiYioJJi8oCADvwSMbog7kQbsO8SAS0REJcKAWxDfEHUR4MnVpoiIqOTYpHwTgZwWREREpYA13ILEXgB2zELd9AwA3RCdnI6MrGwY9TxHISKiomP0KEh6MrDtCzgf/hl6Bx1kKeXopPRiFDEREVExA+7Fixdx6dIly+2dO3fixRdfxOzZsyvX4hedxkLX43X4u3J5RyIiKoeA+8gjj2DdunXqemhoKHr16qWC7uuvv44333wTlYKjG9DnbaDjaPh7uqi7ODWIiIhsGnAPHz6M9u3bq+uLFi1C8+bNsXXrVsybNw9z585FZR04xeUdiYjIpgE3IyMDTk6mIPT333/jvvvuU9cbN26Mq1evotJIjgYu7UYjx0h1kzVcIiKyacBt1qwZvv76a2zatAlr1qxB37591f1XrlyBn58fKo0N7wFzeqJH0gp1kwGXiIhsGnDfe+89zJo1C927d8fQoUPRqlUrdf/vv/9uaWquTMs7BmTlLH7B9ZSJiMiW83Al0EZGRiI+Ph4+Pj6W+0eNGgVXV1dUGl6mNH0+GaHqkjVcIiKyaQ03JSUFaWlplmB7/vx5zJgxAydOnEBgYCAqW8B1S2XAJSKicgi4AwYMwI8//qiux8bGokOHDvjoo48wcOBAzJw5E5UtEb0xOQxGZCI8gSn6iIjIhgF379696Nq1q7q+ZMkSVKtWTdVyJQh/9tlnhX6d6dOno127dvDw8FA1YwnYUku2G67+gN4JOmiopotGcnoWktIyy3uviIioqgTc5ORkFSTFX3/9hfvvvx8ODg7o2LGjCryFtWHDBowZMwbbt29Xo51lulHv3r2RlJQEu+DgAHjlSUTPWi4REdkq4NavXx/Lli1TSzyuXr1aBUkRHh4OT0/PQr/OqlWrMGLECDXNSEY6y6IZFy5cwJ49e2Bv/biNnWPVJUcqExGRzQLu5MmTMWHCBNStW1dNA+rUqZOlttu6dWsUV1xcnLr09fXN93EZqCUjo81bQkICbBVw67GGS0REtp4WNHjwYHTp0kWtKmWegyt69uyJQYMGFWtHsrOzVQKEzp07q6UiC+rznTZtGspj4FRtvWm1qfD4VNu+PxERVe30fNWrV1e1WVldypw5SGq7srxjcUhfrqzRvGDBggKfM3HiRFULNm9Hjx6FrRa/qK7lLO/IxS+IiMhWAVdqo5IVyMvLC3Xq1FGbt7c33nrrLfVYUY0dOxbLly9XGYhq1TIFuPzI+s3SR2zezAO3bNGk7JcZpi45aIqIiGzWpCxp+L799lu8++67qglYbN68GVOnTkVqairefvvtQr2Opml4/vnnsXTpUqxfvx716tWD3cmp4bplRMseM+ASEZHtAu4PP/yAOXPmWLIEiZYtW6JmzZp47rnnCh1wpRl5/vz5+O2331RtVXLrCqk5u7iYctCWO5+6wMsnsPmSBvywl03KRERkuybl6OjofPtq5T55rLBkVSrpi5W1mWvUqGHZFi5cCLvhoAc8qiPAw7RGNJuUiYjIZgFXRiZ/8cUX190v90lNt7CkSTm/Tebm2ptAT1P+38jEdGRla+W9O0REVBWalN9//33069dPJZ83z8Hdtm2bWghjxQpT7thK5cACBBxfgXv1wVie1QkxyenwdzcFYCIiojKr4Xbr1g0nT55Uc24leYFssrzjkSNH8L///Q+VTthhOBz7DR0dz6mbbFYmIiKb1HBFUFDQdYOjDhw4oEYvz549G5VKo36AZ03s3GwEUkwBt0mN8t4pIiKqEgtfVCl1OgEdn0WMTwt1kzVcIiIqKgbcIgjI6bdlXlwiIioqBtzC0DTg0m50zdgMR2SwhktERGXbhysDo25EBk9VWj8OwKD0RHym+wgRiXXKe2+IiKgyB1xZAepmjz/++OOodHQ60xKPEccRpItERAIzBhERURkG3O+//x5VliQxiDiOmrpI7ElIK++9ISKiCoZ9uEVMYlBTF8U+XCIiKjIG3CImopcabnxqJlIzsope2kREVGUx4BYxL67UcAXn4hIRUVEw4BYx4AbrI9VlRCL7cYmIqPAYcIvYhxuoRUGHbNZwiYioSBhwC8ujBqDTw4hM+COOAZeIiIqEAbew9AbAM0hdraXm4rJJmYiICo8BtxjNykG6KK6nTERERcKAW6yRyhGs4RIRUZEw4BaFVy1kOxjhqkvjKGUiIioSBtyi6PYaDgw/gRmZgxHJPlwiIiqrtZSrPKMzAjyzVTHIoClN06CTxAZEREQ3wRpuEQV4mJLQp2dlIy4lo6h/TkREVRQDblFkZcLp15H4w3ky3JHMgVNERFRoDLhFnYt7ZgNa4LSaGsS5uEREVFjswy2qez7AR+su4epVP45UJiKiQmMNt6haDsH5wB5IgCtruEREVGgMuCUYOMUmZSIiKiw2KRdV/FV0TNmIMw6RCE+oWeQ/JyKiqok13KK6sA29jryG5wy/sYZLRESFxoBbVN7B6qImMwYREVERMOAWM2NQNcQgJiGpyH9ORERVEwNuUbkFQtM7Qq/TYEwOQ0aWaalHIiKiG2HALSoHB8CzpqVZOTKRieiJiOjmGHCLQWeViJ5Tg4iIqDAYcIuDA6eIiKiIGHCLI6eGy5HKVJo+XnMSfWdsxOHLcSxYokqIAbc4vGqrCwZcKi37LsTgs7WncDw0AcO/24nT4YksXKJKhgG3OKz7cDloikpI0zS8tfyoum7U6xCVlI7H5uzAxehkli1RJcKAW8I+3PC41FL+L6Gq5o+DV7H3QixcHfX44/kuaBDojtD4VDw6ZwfC4nl8EVUWDLjF4RmkLlx1aUhJiCzl/xKqSlIzsvDeyuPq+rPdbkHj6p746akOCPZ1xYXoZFXTjU5KL+/dJKJSwIBbHEYXpLkF4Vx2NWQkRJXG/wNVUXM2ncHl2BQEeTnj6TtC1H3VPJ0x76kOqObphFPhiapPNyE1o7x3lYhKiAG3mEJH7kL39E+wL8lP9cERFVV4fCq+Wv+vuv7a3Y3hbNRbHqvt66qCrq+bIw5djsOTc3cjJT2LhUxUgTHgFpO/h7O6TMnIQhJ/CKkYPvzrBJLTs9A62Bv3tTJ1U1irH+iBH59oDw9nA3aei8YzP+1BWiaDLlFFVa4Bd+PGjejfvz+CgoKg0+mwbNkyVBRuTga4OeotNRWiopC5tov3XFLX37i3qTr+89O8phe+H9EOLkY9Np6MwIsL9iOT63cTVUjlGnCTkpLQqlUrfPnll6hwTv6FJYZJeMcwh8s7UrGmAUlPhNRsbwv2ueHz29b1xezH28BR74CVh0Px2i+HkJ3NbgyiisZQnm9+9913q61CykpHk+xTSHPIwiXOxaUiWH0kDDvORsPJ4KD6bguja4MAfDa0NcbM34tf9l6Cu5MeU+9rVmDNmIjsT4Xqw01LS0N8fLxlS0hIKL+dqd0es2q8hdcyRrGGS4UmfbDTVx5T10fdEYKa3i6F/tu+zavjwwdbqus/bDuPj/46yZInqkAqVMCdPn06vLy8LFvTpk3Lb2fcA3G1Rk+c0IIZcKnQftx6HuejkhHo4YTR3W659kBWJhBlGrF8I4Na18JbA5ur61+sO42ZOaOcicj+VaiAO3HiRMTFxVm2o0dNy+GVlwAPJ3XJFH1UGFGJaWq9ZDGhTyM18M5i9URg1h3AydXX7svMP9fysI518J+cpuj3Vh3H/7ad438AUQVQoQKuk5MTPD09LZuHh0e57k/zlN14Sv8n9NGmH1GiG/nk75NISMtEsyBPDL7NtB63svMbYOdsID3xWpA9swH4vA1wdlO+ryW147E96qvrb/x2BL/uNY14JiL7VaECrr1pfuEnTDLOQ7W4A+W9K2TnToYlYP6OC5ZpQA4OOYOd/v0HWPma6XrPyUDT+0zXN34AxF0EDi0u8DVf7t0QI26vq66/suQgVh2+WtYfg4gqasBNTEzE/v371SbOnj2rrl+4YPphsnc6b1OaPvdU/tDRzacByUyevs2qo2OIn+mBiJPAohGAlgW0Ggp0GX/tj4YuALpOAPpOL/j40+kw+d6mGNymFrKyNYyZvw9Lcub2EpH9KdeAu3v3brRu3VptYvz48er65MmTURE4+pmyBvlkhKkfPKL8rD8RgU2nItU82on35EwDSo4G5g8B0uKA2h2B/p9KBL32R07uQM83AEc30+3sbGDhY8CBBRLBLU+TmvK797fA/a1rqmNwwuID+HrDv1xulMgOles83O7du1foHwYXf1NzXhAiVUYX8yAqIrOMrGz835+mwX0jOtdFHT83IDPdFDxjzppSPT48DzDc5NiRpuVjf5i2038D/T4CnL3UQwa9Az58sJU6/mZtPIN3Vx5HeHwaJvVrcq3pmojKHftwS0DvE3wtEX1C/iNKqWqTftt/I5Lg5+aIsXfWN9VO/3wJOL8FcPQAHlkEuPnf/IVaDAZ6TAJ0elPw/boLcHGn5WEJrBPvaaKCrPhuy1m8uHA/0jOzy/LjEVERMOCWRE4fbg1dFMLjk0v0UlT5xCVnqJHJ4qVeDeHpbAS2fg7s+wnQOQAPfg8EmgLkTTnogW6vAE+sMtWKYy8A3/UFNrwPZF9LaPBU1xDMeOhWGBx0+P3AFTz5wy4kpmWW1UckoiJgwC0JjxrIhgOcdJlIiLxSopeiyuezf04hNjkDDau54+F2tYHjK4A1OeMT+kwHGvQq+ovWbg+M3gy0eNA02Grd28Dce4HYi5anDGxdE9+OaAdXR73qO37km+2I5PKjROWOAbck9EbEG0wjTtOiz5fSfwlVBmciEvHDVtOCFJP6NYUh5gzwy1MyZhlo+wTQ4Zniv7j03T4wBxg0G3B0By5sBb7uDGz9Agg9rAZYdWsYgJ+f7qjy6R68FIfBM7fiYjRbYYiq7KCpyiDBuQa8EyOgxVyrYRC9s+I4MrM13Nk4EHc0DACyfIDWjwERx4G73889Irm4Wj0E1G4H/PI0cHk38Nfr1wLyHa+i1e1jsWR0Jzz+3U6ci0rG/TO3Yu7IdmgWZBpsRVWQtIQkhgNetQCPaqb7Ys6ZxgWkJQJpCYCzJ9BpbOHGFlCRMOCWULpbEJB4EIaEyyV9Kaoktp6OxN/HwlQ/6n/vyemj1RuAe943rSSlN5bem/mGmPp1d80BTv0FXNgBpMYBTqZV2EIC3PHbgz64OH8KViQ3w8OzsjD78bbodEvOXGCqXFJiTWMEDi0CkiKB9CTgtXPXTvBkCVEZ6S6j3Ns9lROELwD//F/u15HpZ9KKUreL7T9DJcaAW0LZXrWBMMCYeFn1k8loVKZMq9rZgN5cbpoG9FiHYNS/9CvgN/RakL3Z9J/ikNfu+KxpkyQIoQdNA6ty+EXsgl/mQejcDJgdn4nh3+3EjIdvxT0x84DApkBwR8DVt/T3i2xHFlHZOQvY/zOQkZT7sYwUwNHVdN2jBuAVDOgdrz0utd3Ww0wnadJFcXQZEHkS+KE/0O014I5XTIP2yoqmATu+Bur3AvxNy5VWVjqtAk+EvXTpEmrXro2LFy+iVi2rtWlt6OzKT1Fvx2SsyWqDpzNehrPRQaVcq+njqi5r+Zg2030uCPRwhr4qz42UBRwcKvbQAfnKRCSm4UxEUs6WiDORpsuLMSlqAQovFyO2374HLlveBW7pCTz2S+k0IxeH1GBO/410Z3+M219TJbEP0MVil9Nz157j5KkyYMG9mtUWeP11t4BS+f/LztZUcfDktESFaJqTLcHq37XX7g9oAnQYBQS1Nk09861XtIApteIVrwD755lu1+0K3P8N4FkDZWbddNMI/vs+M02Bq6SxiDXcEqoZ0hThe4KQqPMBMgBjRiKWxz8OLV6HW0/ORjpMNZv/M3yLOvpdiJJxag56ODg4wMFBD53eiExHT2Q5eSPb2Qc6V1/o3XxhcPeDoVoTuDTuCaM+5wcuNd50FlqYH245j8rOBLLSTc2Y1pdSy7KqASElBnAwAkbXQv+YatnZSEuKQ0r0ZaTFXkVGXCiSdS64EnCHmoaSlJqBuzYPgWtqGOY0+wGhmi/kPKNP1E/oeHku0p18kensC7j6QefmD6OHPxw9A6F39wdcZfMz9SHJpbO37YK0+fxTp0NqRhbORVkF1Ygk/JsTWBNSC55q4+lswPT7W8DFMRvY5Q40HVB+wVbI/3XbJyB1mi+aapjy+2Gs2x6B+Zk90NfjDHxTzgNp8aYt6vSNX2vUBiDoVtP1o78Bx/8EGvS+9iOZngwcXKBqSprRFbFZjricpMeFROBsPHAqRsOx6Gz8G52hFuyQxToC3J3Uiai6bt7kPk/TdT83JzgaHMouaIUfNfVdSvIIVQ45fZnW90kQMroALr6Aiw/Q6B4gMGfVMHlMmvHlMaMzypzs1/75wI5ZQLQ5PaPOtE8yGK/eHSU73mR1s4FfmV5n+Xjg3CbTvO9Bs4AGd5XOZzj5F+DmB9RsY/rOyXddauYVt/5XKAy4JeTYqBcCJx3DIAD3ZGYhLDQULnPS1WPP9miAi3EZuByTgmphaQjIjjf9kRxTWTlbhgTS/F97VVY7jM5Ih4tRDw8nPbZkPgwHZGN80DxkuVdXP0J9Qr/BbQn/QJ+dCb2WDoOWAaOWAQMy4KDe6HrbdK3xonGS5fZf6cPhhUQ8bPwM5x1MZ2ePZ/6CB7P+RLrOERkwqhMH2eQVfbRY+GmxcNZlwPrnZWd2I4xMv5ZQfbtTGFx1MViz4yAOaSHqvhDDOXQ1pMKYcgWQLebmZXzerQW+bzTLcvv5I0PgkRGJbxvOQphrA3Vfp7CfcUfo3JzidYCW84OjyQ+Rlg2dXMt1mY1QfQ2M95upVoOSBSK+jH8et2SfwwuGydistURMcjq66/ZhnOFXGDVv+GpeqANvtNC8EKH3hs49EG5+NeFbrTaCq/nhFn831WdazdMpp+ZWA3h+D+BRHfZCWlfeGtAcn3s4479rAvDfGGBEG18MbeII5/RIOKdGwik1EsbUCBiTI6BPDodDUjh0MtAmKcJUyzWThTcOLkS6SwBO+vZStfyIi6fw5O6X1MNSAj45mymD7zWZRgdkQQ/Ib2wS8MDFaVilmVZukwxcAwxL8GtWF0zKfFLdV9MlE2u0Z0y1YvXi6n9SvYuW32251OmwMHgqznq0UX/XMmYt+l3+BGe9OuLvJm/ByaCHswEY+Xfn4p3EmAOuJKCQlcNqtQeeWnPtOYtHqmNPBROfeqb+dtl86hY/MG+eAWz8EEhPMN128gJuG2bqj5WabGlq9bApIMrnCDsEzHsA6DwOuPON4o9DiDkHrJoInFgBVG8JjFpvqn3LyH3PmkDje649VyoIZdEFU44YcEuRfIGDg6oDLx5SX7SXvOtcO9OMm43MpGhEJaUhIi5ZLZQRGZ+CmIQkaCmx0KfGwJAeC8f0OLhkyhaPPdmmBOUpGVnQZSTB6GyqVa0+k4pUmBImtDVcQYDh5nOAszUd0mFQQTMuy4CwlGsrYzk6patfsUuJGq5qpujvYIiFvyHWdHKQn5yPFa+5IBI+iHHwwXmnEDQP8ISbowHuTgbMz/4/6J3c0NmzLnq6uKnF+68kTMCbiY8jOykS+pQo6FOj4ZweA9fMOPgiHj66BPjp4uGLBPjqEuCpS0ZkQirm5kyxEc86JcJRl4o/DlzGUc30xXfTh6GPMedHqJCy01Ow5/y1iJ/qqKmJchJoo7NNJ02NnSJwKwpI8i5FeCVnkx8+aXaVvlD5QarX1fQcOwq2ZnIy8ELPBvB3d8KkZYcwd0805u4xP+qds+XuS5PWCReDBv3HB+BoOAIngwPa6WogRP84tm7yxvYNm9XzqiEatY1t4II0uOlS4Yo0eOrT4K5Lg4uWAqNmKleDLhsGXFsF69nuITihC1ErtjW+6AzX2DR4OQKGbJ0a7R2Xkg5X55Rrx2MhK0Kbjl/BhuwAdf0h/SUMMcYiKvwKZly+llKzh2M19XKJcEESXJCgyaUzEjUXdZ9cJsMZXvp0hLhnoLZLKqIivRAclqBOsPRSw5UVwKTma+3ESiAzJb//AVO/qa9VEPa95dp162AsNT4J2uYmYQlAEmz9Gphqs5L0QtbdLiv+DYCn/jaNgJeBeVs+Ne1T77eK9jrSj7zlU2DzJ0BmqvzAACHdgawM02eTzTrYJkYAc3oCtz9vOpkozxaiUsQ+XDsmNa/E1EzVfBmfmoGEpESkxUciQsJRaibSs7Lhm3oB7pmx0BmdoNM7Qmdwhs7gCL3cNjpBb3SGg9w2GGE06NXIWYODQ+7jV5qeM9OhGZzVCkgaNBiSw6FPjoCDNEFnpalLXbZsWTB6BsLRuwacfWvAzc2zVPqkpd8zLiUDscnpiEnOUD+wsmhEXGIyUhJikWy4NpXFIy0UDloWEh0DkOUgg9QA54xYdaKiU01SUtfJVj/K8mn0BgMMej30egMMUgYGBxjkutERWe41VFIBabZ3zoyDUQ8YnD1hcHRWc1j9s8KgCz0EJIaZplPkugwDEsJU+eQizeCPLAZqtYG9W3M0DB+vOYm45HR1PKVlmmr7cr0orXsyWDAkwA31/GVzV9dD/N1Q29cVzlKoZjKoS5oOJUhZrZClas6GnIE80jwro20d3ZHt4ovYlAxExKcg/uq/iE5OQ2xSuvrRl3+Wy2wtZ112U4AyD02Jc6yBdIOr+ixOGbFwTg1HrM4LUfBWn1W6Dawv0/Lctr7MLz+JtD41DfJEixruaFHDBU2Dq6FBoLv6nuHQEiAl2nScRJ8Fos+YNmmiLsgD315rnr+8F1g6GujyEnDr0Gtlc2kXEHJnqXezyHdw7bEwnApPxGMd66hxCLlIF8K6d4ARfxZtypCceEgKytictQrqdQPu+QAIaFTw38gKarKoi2jSH7jv8+tPaCpgHy4DLlFJyC+5/ICag3BCKJAaCzQZALibalYVkQQsqVmmWwVgubQOyHIp3Rr1/Nzg5VqKU53sUGZWtmoyP3QpDoevxOHw5TgcuRKP5HSrk4YcUvtvXMMTzSUQ1/RCq9reaFzdw9TNIMdLcpQp8Eb9ey0Iq+1fYNhSUzOu2P41sOo1oE5nYOSKMvtsKelZWLLnIr7dfFbN1xbSLfLu/S3Ro3Fg7ifLSZL1AKyDi4Am9+XfRC6faeV/gFOrTbc9goC+7wBNB968xirltP0rYM0UIDvDNLJ68LemldbsEAMuEVEZkhrh2cgkHLkSZwnERy7HIyGftasl6I7qGoI+zaqpwWI3G7CnnFpj6vNsOcSSGao0hSek4set5/HTjvOqNck84M/TxYhLMaamcMm1/Ma9Ta+v7YoDC4Glo4Aat5qanc39ujJwTpqOpQlZWn9kQGanMabpRUVt/pZa/pKRpnKQZntJWXn7OLub6cCAS0RkYzLd6UJ0Mg5dvlYT3nUuxpK1SaYIPtmlHoa0rQ03p/IZQnMiNAFzNp3Bb/uvqJYKEezriic618WDbWvDQafDR3+dwLdbzqpzgAJruzIl6ddRQPtngO6vmU4YZNS6DIqKu2B6TkgPU/Ox9AUXlzSj//EicORX022ZZicjpu2oBYkBl4jIDsiCOP/bdh7/235e5c021yQf7VgHI26vi2qezjbpIth8OhLfbDqLjScjLPffFuyNp7uGoHez6teNxdh9LhqvLDmoavEF1nalK0XGLEgzsywb+VlrUxOwZy1T87E0N5fGgCdNA/b+aOoLloFo0ud//2zTwKuikClf8ZeBuIumgXelNM2JAZeIyI5IX+kvey+pvlJzEDPqdRhwa00V9BpVNy3HWZqkZi1pGqVGezzUNIpfzYdvVl2lcmxTx+em+1yo2q7ZP2+bslh1fdk0n7e0hR8DFo8wrUkuo73lfbpPNC2dKmTUs0xXk6AqGbXMwV4C9YGfTbVl6wVCxmwvld1iwCUistNmZ1lr+5tNZ1Rzs5kkuZB+3s71/Uq8ApeM9p+344LKWBWeYBpFL+kapSn7ic71EOyXs9RjIRWqtmsr6cmmwWRS4wUQVf9+OA6eDQ/JN52RCrydM1f81bPXliyVlbN2zjZdl/5wqYHLKGnJSV0KGHCJiOzcvgsxmLPpLFYevmqZdiQjmkfdEYJ7WwZdt8KW1FhlWVGZrxwen6quh8en5bqMiE9VQVZGmQuplY64vR4eaR9cotHkeWu71T2d1YpqBdZ2S2Fa5NXYVFyMSVb94rJdNG8xKeicsh7vGL/FgewQPJbxuuofl7J7M2IcHJ3dkdD3M9Su19A0SC3mvGkusFdNS2KP0sSAS0RUQVyISsZ3W85i4a6LaqEbc6DsUM9PLZZjDqbm0cSF0aSGJ57uWi/fwF0SpVXbTc3IUp8rND5VbeZgqgJrTDKuxKaqkeA30tA1CSEOV7EqMf+kB/K56we4o3ENDxWMG1X3RJPqHmrJ0NJcx5sBl4iogjE3BcvKalKLzY/0+8pa0wGeztfWnM516YxADyfU8HIus+QQN6rtygAtWbwmNC4VYTnBNL/rMYU4eZA5zVJzlVHUstU2bz5y6WJqRs4pNxl9fdyyxeNkaAKS8pknLXxcjWhc3VMF4jf6NYVDCRfvYcAlIqrAaR5XHQ5VwckUSJ0tAdXb1Wg3WZby1nYlK5rUxM3ToAoTUKt7OauR2uYgah1c5fMWNxhKX7nMJ5bgK0HYFJDj1b6aK86yv1v+cydKitmCiIgq8LrsMnrZ3rWt64sVL3S11HYvx15bO1qWRpVAWt3TyRJUa+Rcym2pFUszdFmdPEiglsFhssm0J+um7NPhiTh2Nf6mTdaljckLiIio2Fwc9Zh0b1O1/rLUbiWQSm1cThrskbNRj+Y1vdRmawy4RERUYnX93dRGBbOvBSmJiIgqKQZcIiIiG2DAJSIisgEGXCIiIhtgwCUiIrKBCj1KOTvbNLn66tWr5b0rRERURV3NiUHmmFQpA25YWJi6bN++fXnvChERVXFhYWEIDg4u8HGdJgtfVlCZmZnYt28fqlWrBgeHkrWOJyQkoGnTpjh69Cg8PEo/m0Rlw/JiefEYsy/8TpZfeUnNVoJt69atYTAYKmfALU3x8fHw8vJCXFwcPD09y3t37B7Li+XFY8y+8Dtp/+XFQVNEREQ2wIBLRERkAwy4OZycnDBlyhR1STfH8ioallfRscxYXpXt+GIfLhERkQ2whktERGQDDLhEREQ2wIBLRERkAwy4Ob788kvUrVsXzs7O6NChA3bu3GmL8q9wNm7ciP79+yMoKAg6nQ7Lli0r712ya9OnT0e7du3UxPrAwEAMHDgQJ06cKO/dslszZ85Ey5Yt1bxI2Tp16oSVK1eW925VGO+++676Xr744ovlvSt2a+rUqaqMrLfGjRvb5L0ZcAEsXLgQ48ePVyPW9u7di1atWqFPnz4IDw+3yX9CRZKUlKTKR05Q6OY2bNiAMWPGYPv27VizZg0yMjLQu3dvVY50vVq1aqmgsWfPHuzevRt33nknBgwYgCNHjrC4bmLXrl2YNWuWOmGhG2vWrJla/9i8bd68GTYhK01Vde3bt9fGjBljuZ2VlaUFBQVp06dPL9f9sndy+CxdurS8d6NCCQ8PV+W2YcOG8t6VCsPHx0ebM2dOee+GXUtISNAaNGigrVmzRuvWrZs2bty48t4luzVlyhStVatW5fLeVb6Gm56ers6m77rrLstJiKzLLLe3bdtmm7MeqjJkGTnh6+tb3rti97KysrBgwQLVGiBNy1QwaUXp169frt8xKtipU6dUt1hISAgeffRRXLhwAbZQobMFlYbIyEj1xZYECNbk9vHjx8ttv6jykQXOpW+tc+fOaN68eXnvjt06dOiQCrCpqalwd3fH0qVL1SLzlD85KZGuMGlSppuTMTpz585Fo0aNVHPytGnT0LVrVxw+fLjME9dU+YBLZMtaiHypbdZfVEHJD+H+/ftVa8CSJUswfPhw1RfOoHu9ixcvYty4cWp8gAz4pJu7++67Ldelv1sCcJ06dbBo0SI8+eSTKEtVPuD6+/tDr9dbcuuaye3q1auXaeFT1TF27FgsX75cjfKWgUFUMEdHR9SvX19db9Omjaq5ffrpp2pAEOUm3WEyuPO2226z3CctdnKcffHFF0hLS1O/b1Qwb29vNGzYEKdPn0ZZq/J9uPLlli/12rVrczX9yW32G1FJydgyCbbSLPrPP/+gXr16LNQiku+jBA66Xs+ePVUTvLQImLe2bduqfkm5zmB7c4mJifj3339Ro0YNlLUqX8MVMiVImq3kQG3fvj1mzJihBmqMHDmyzP8DKuLBaX0mePbsWfXFlkFAwcHB5bpv9tqMPH/+fPz222+qfyg0NFTdL3k4XVxcynv37M7EiRNVk58cS5IgXMpu/fr1WL16dXnvml2SYyrveAA3Nzf4+flxnEABJkyYoNYSkGbkK1euqOmgcmIydOhQlDUGXAAPPfQQIiIiMHnyZPWDeOutt2LVqlXXDaQiqLmRPXr0yHWyIuSERQYi0PULOYju3bvnuv/777/HiBEjWFx5SPPo448/rgazyEmJ9LFJsO3VqxfLikrFpUuXVHCNiopCQEAAunTpoubJy/WyxmxBRERENlDl+3CJiIhsgQGXiIjIBhhwiYiIbIABl4iIyAYYcImIiGyAAZeIiMgGGHCJiIhsgAGXiIjIBhhwiahQdDodli1bxtIiKiYGXKIKQJaBlICXd+vbt2957xoRFRLXUiaqICS4yhrM1pycnMptf4ioaFjDJaogJLhKjmbrzcfHRz0mtV1JlCCZdiQLUUhIiErebk3SuN15553qcckmM2rUKJX9ydp3332HZs2aqfeSdGWSWtBaZGQkBg0aBFdXVzRo0AC///675bGYmBiVFk4WgZf3kMfzniAQVWUMuESVxBtvvIEHHngABw4cUIHv4YcfxrFjx9Rjkm6yT58+KkBLQvfFixfj77//zhVQJWBLOkEJxBKcJZiaE8GbTZs2DUOGDMHBgwdxzz33qPeJjo62vP/Ro0excuVK9b7yev7+/jYuBSI7phGR3Rs+fLim1+s1Nze3XNvbb7+tHpev8ujRo3P9TYcOHbRnn31WXZ89e7bm4+OjJSYmWh7/888/NQcHBy00NFTdDgoK0l5//fUC90HeY9KkSZbb8lpy38qVK9Xt/v37ayNHjizlT05UebAPl6iCkDzE5vy6Zr6+vpbrnTp1yvWY3N6/f7+6LjXOVq1aqeTkZp07d0Z2djZOnDihmqQlGXfPnj1vuA+Sn9ZMXsvT01PlsBXPPvusqmHv3bsXvXv3xsCBA3H77beX8FMTVR4MuEQVhAS4vE28pUX6XAvDaDTmui2BWoK2kP7j8+fPY8WKFVizZo0K3tJE/eGHH5bJPhNVNOzDJaoktm/fft3tJk2aqOtyKX270pdrtmXLFjg4OKBRo0bw8PBA3bp1sXbt2hLtgwyYGj58OH766SfMmDEDs2fPLtHrEVUmrOESVRBpaWkIDQ3NdZ/BYLAMTJKBUG3btkWXLl0wb9487Ny5E99++616TAY3TZkyRQXDqVOnIiIiAs8//zyGDRuGatWqqefI/aNHj0ZgYKCqrSYkJKigLM8rjMmTJ6NNmzZqlLPs6/Llyy0Bn4gYcIkqjFWrVqmpOtakdnr8+HHLCOIFCxbgueeeU8/7+eef0bRpU/WYTONZvXo1xo0bh3bt2qnb0t/68ccfW15LgnFqaio++eQTTJgwQQXywYMHF3r/HB0dMXHiRJw7d041UXft2lXtDxGZ6GTkVM51IqqgpC916dKlaqASEdkn9uESERHZAAMuERGRDXDQFFElwJ4hIvvHGi4REZENMOASERHZAAMuERGRDTDgEhER2QADLhERkQ0w4BIREdkAAy4REZENMOASERHZAAMuERERyt7/AxhSWivJTNTvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curves\n",
    "from previous_chapters import plot_values\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ddb5e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 82.06%\n",
      "Validation accuracy: 79.33%\n",
      "Test accuracy: 77.83%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy on the full dataset\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
