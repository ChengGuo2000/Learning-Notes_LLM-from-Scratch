{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cc53c7",
   "metadata": {},
   "source": [
    "# Appendix E\n",
    "I am using the UCI Sentiment Labelled Sentences dataset instead of the UCI SMS Spam Collection dataset to get some novel results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff771e",
   "metadata": {},
   "source": [
    "# E.2 - Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a47774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "import pandas as pd\n",
    "from previous_chapters import random_split\n",
    "data = []\n",
    "with open(\"../chapter-6-learn/sentiment_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        text, label = line.rsplit('\\t', 1)\n",
    "        data.append((text, label))\n",
    "balanced_df = pd.DataFrame(data, columns=[\"Text\", \"Label\"])\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"../chapter-6-learn/train.csv\", index = None)\n",
    "validation_df.to_csv(\"../chapter-6-learn/validation.csv\", index = None)\n",
    "test_df.to_csv(\"../chapter-6-learn/test.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4448336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating PyTorch datasets\n",
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"../chapter-6-learn/train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"../chapter-6-learn/validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"../chapter-6-learn/test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a4dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PyTorch data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True,\n",
    "                          num_workers = num_workers, drop_last = True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size,\n",
    "                        num_workers = num_workers, drop_last = False)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size,\n",
    "                        num_workers = num_workers, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65263cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 103])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Print the batch dimensions\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533fe72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 training batches\n",
      "38 validation batches\n",
      "75 test batches\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a08dfe",
   "metadata": {},
   "source": [
    "# E.3 - Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fefa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 16:21:41.407852: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Loading a pretrained GPT model\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"So far, I had\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size = model_size, models_dir = \"../chapter-5-learn/gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b88dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, I had no idea what to expect.\n",
      "\n",
      "I was so excited to see the\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model generates coherent text\n",
    "from previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text\n",
    "text_1 = \"So far, I had\"\n",
    "token_ids = generate_text_simple(model = model, idx = text_to_token_ids(text_1, tokenizer), \n",
    "                                 max_new_tokens = 15, context_size = BASE_CONFIG[\"context_length\"])\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834997d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for classification fine-tuning\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features = 768, out_features = num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63716271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 50.00%\n",
      "Validation accuracy: 48.75%\n",
      "Test accuracy: 52.50%\n"
     ]
    }
   ],
   "source": [
    "# Calculate initial classification accuracy\n",
    "from previous_chapters import calc_accuracy_loader\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches = 10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches = 10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches = 10)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b927cd",
   "metadata": {},
   "source": [
    "# E.4 - Parameter-efficient fine-tuning with LoRA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
