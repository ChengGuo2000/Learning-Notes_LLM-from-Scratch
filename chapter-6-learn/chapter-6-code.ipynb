{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d49f34d",
   "metadata": {},
   "source": [
    "# 6.2 Preparing the dataset\n",
    "I used the UCI Sentiment Labelled Sentences dataset to create some novel results, which is provided by by Dimitrios Kotzias, Misha Denil, Nando de Freitas, Padhraic Smyth in 2015 in the paper titled \"From Group to Individual Labels Using Deep Features\" published in Knowledge Discovery and Data Mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c37a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to download the dataset\n",
    "# import requests\n",
    "# import zipfile\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# zip_path = \"sms_spam_collection.zip\"\n",
    "# extracted_path = \"sms_spam_collection\"\n",
    "# data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbe2e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for downloading the dataset\n",
    "# def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "#     if data_file_path.exists():\n",
    "#         print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "#         return\n",
    "#     response = requests.get(url, stream=True, timeout=60)\n",
    "#     response.raise_for_status()\n",
    "#     with open(zip_path, \"wb\") as out_file:\n",
    "#         for chunk in response.iter_content(chunk_size=8192):\n",
    "#             if chunk:\n",
    "#                 out_file.write(chunk)\n",
    "#     with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "#         zip_ref.extractall(extracted_path)\n",
    "#     original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "#     os.rename(original_file_path, data_file_path)\n",
    "#     print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "# try:\n",
    "#     download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "# except (requests.exceptions.RequestException, TimeoutError) as e:\n",
    "#     print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "#     url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "#     download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68bb8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "# download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d28cf92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee9d5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the class label distribution\n",
    "# print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13fca2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for creating a balanced dataset\n",
    "# def create_balanced_dataset(df):\n",
    "#     num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "#     ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "#     balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "#     return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea76700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a balanced dataset and convert class labels\n",
    "# balanced_df = create_balanced_dataset(df)\n",
    "# print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c137f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the label\n",
    "# balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "# balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4bab385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Label\n",
       "0                              Wow... Loved this place.     1\n",
       "1                                    Crust is not good.     0\n",
       "2             Not tasty and the texture was just nasty.     0\n",
       "3     Stopped by during the late May bank holiday of...     1\n",
       "4     The selection on the menu was great and so wer...     1\n",
       "...                                                 ...   ...\n",
       "2995  The screen does get smudged easily because it ...     0\n",
       "2996  What a piece of junk.. I lose more calls on th...     0\n",
       "2997                       Item Does Not Match Picture.     0\n",
       "2998  The only thing that disappoint me is the infra...     0\n",
       "2999  You can not answer calls with the unit, never ...     0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the sentiment dataset\n",
    "import pandas as pd\n",
    "data = []\n",
    "with open(\"sentiment_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        text, label = line.rsplit('\\t', 1)\n",
    "        data.append((text, label))\n",
    "balanced_df = pd.DataFrame(data, columns=[\"Text\", \"Label\"])\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf5d3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for splitting the dataset\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac = 1, random_state = 123).reset_index(drop = True)\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7469c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset and save the parts to CSV\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index = None)\n",
    "validation_df.to_csv(\"validation.csv\", index = None)\n",
    "test_df.to_csv(\"test.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecaa528",
   "metadata": {},
   "source": [
    "# 6.3 Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8566983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and add special padding token\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special = {\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a PyTorch Dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            self.encoded_texts = [encoded_text[:self.max_length] for encoded_text in self.encoded_texts]\n",
    "        self.encoded_texts = [encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af8ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "# Load the train data with the SpamDataset class\n",
    "train_dataset = SpamDataset(csv_file=\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f646739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation and test set\n",
    "val_dataset = SpamDataset(csv_file = \"validation.csv\", max_length = train_dataset.max_length, tokenizer = tokenizer)\n",
    "test_dataset = SpamDataset(csv_file = \"test.csv\", max_length = train_dataset.max_length, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a04232",
   "metadata": {},
   "source": [
    "## Exercise 6.1 - Page 179\n",
    "This exercise is finished by the end of this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe867e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PyTorch data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True,\n",
    "                          num_workers = num_workers, drop_last = True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size,\n",
    "                        num_workers = num_workers, drop_last = False)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size,\n",
    "                        num_workers = num_workers, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312d3445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 103])\n",
      "Label batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Print the tensor dimensions\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d03c078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 training batches\n",
      "38 validation batches\n",
      "75 test batches\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5cf6a",
   "metadata": {},
   "source": [
    "# 6.4 Initializing a model with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a783263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employ the same configurations as before\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"So far, I had\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d64efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 14:12:19.547049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../chapter-5-learn/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Loading a pretrained GPT model\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size = model_size, models_dir = \"../chapter-5-learn/gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de665e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, I had no idea what to expect.\n",
      "\n",
      "I was so excited to see the\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model generates coherent text\n",
    "from previous_chapters import generate_text_simple, text_to_token_ids, token_ids_to_text\n",
    "text_1 = \"So far, I had\"\n",
    "token_ids = generate_text_simple(model = model, idx = text_to_token_ids(text_1, tokenizer), \n",
    "                                 max_new_tokens = 15, context_size = BASE_CONFIG[\"context_length\"])\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "378c2650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "# Test model classifying capabilities\n",
    "text_2 = (\"Is the following text 'spam'? Answer with 'yes' or 'no': \"\n",
    "\"'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\")\n",
    "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(text_2, tokenizer),\n",
    "                                 max_new_tokens=23, context_size=BASE_CONFIG[\"context_length\"])\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40863ffc",
   "metadata": {},
   "source": [
    "# 6.5 Adding a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6255c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac1a9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd9d0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a classification layer\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features = BASE_CONFIG[\"emb_dim\"], out_features = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a32bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the final LayerNorm and last transformer block trainable\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3d55d",
   "metadata": {},
   "source": [
    "## Exercise 6.2 - Page 186\n",
    "This exercise is finished by the end of this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a143f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Feed the model an example text\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "220d0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# Pass the encoded token IDs to the model as usual\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d31d897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# Extract the last output token\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279b648",
   "metadata": {},
   "source": [
    "## Exercise 6.3 - Page 190\n",
    "This exercise is finished by the end of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0550b09",
   "metadata": {},
   "source": [
    "# 6.6 Calculating the classification loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04796925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Obtain the class label\n",
    "probas = torch.softmax(outputs[:, -1, :], dim = -1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "faab2c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Simplify the code without using softmax\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e631b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the classification accuracy\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ba27dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 38.75%\n",
      "Validation accuracy: 48.75%\n",
      "Test accuracy: 52.50%\n"
     ]
    }
   ],
   "source": [
    "# Try the above function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches = 10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches = 10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches = 10)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8411059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :] # optimizing only the last token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8ffa76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the classification loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b26e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.437\n",
      "Validation loss: 2.704\n",
      "Test loss: 2.314\n"
     ]
    }
   ],
   "source": [
    "# Compute the initial losses\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ded49",
   "metadata": {},
   "source": [
    "# 6.7 Fine-tuning the model on supervised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fa92e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for model evaluation (same as the one in chapter 5)\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d1ffb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model to classify spam\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce743b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.017, Val loss 2.505\n",
      "Ep 1 (Step 000050): Train loss 0.726, Val loss 0.710\n",
      "Ep 1 (Step 000100): Train loss 0.700, Val loss 0.728\n",
      "Ep 1 (Step 000150): Train loss 0.706, Val loss 0.703\n",
      "Ep 1 (Step 000200): Train loss 0.700, Val loss 0.710\n",
      "Ep 1 (Step 000250): Train loss 0.697, Val loss 0.703\n",
      "Training accuracy: 50.00% | Validation accuracy: 47.50%\n",
      "Ep 2 (Step 000300): Train loss 0.708, Val loss 0.728\n",
      "Ep 2 (Step 000350): Train loss 0.684, Val loss 0.704\n",
      "Ep 2 (Step 000400): Train loss 0.679, Val loss 0.697\n",
      "Ep 2 (Step 000450): Train loss 0.668, Val loss 0.665\n",
      "Ep 2 (Step 000500): Train loss 0.705, Val loss 0.629\n",
      "Training accuracy: 67.50% | Validation accuracy: 72.50%\n",
      "Ep 3 (Step 000550): Train loss 0.649, Val loss 0.600\n",
      "Ep 3 (Step 000600): Train loss 0.510, Val loss 0.582\n",
      "Ep 3 (Step 000650): Train loss 0.566, Val loss 0.613\n",
      "Ep 3 (Step 000700): Train loss 0.464, Val loss 0.535\n",
      "Ep 3 (Step 000750): Train loss 0.446, Val loss 0.507\n",
      "Training accuracy: 80.00% | Validation accuracy: 77.50%\n",
      "Ep 4 (Step 000800): Train loss 0.466, Val loss 0.488\n",
      "Ep 4 (Step 000850): Train loss 0.388, Val loss 0.464\n",
      "Ep 4 (Step 000900): Train loss 0.284, Val loss 0.445\n",
      "Ep 4 (Step 000950): Train loss 0.395, Val loss 0.437\n",
      "Ep 4 (Step 001000): Train loss 0.192, Val loss 0.449\n",
      "Training accuracy: 90.00% | Validation accuracy: 77.50%\n",
      "Ep 5 (Step 001050): Train loss 0.235, Val loss 0.427\n",
      "Ep 5 (Step 001100): Train loss 0.367, Val loss 0.409\n",
      "Ep 5 (Step 001150): Train loss 0.406, Val loss 0.411\n",
      "Ep 5 (Step 001200): Train loss 0.297, Val loss 0.372\n",
      "Ep 5 (Step 001250): Train loss 0.278, Val loss 0.366\n",
      "Ep 5 (Step 001300): Train loss 0.272, Val loss 0.410\n",
      "Training accuracy: 92.50% | Validation accuracy: 82.50%\n",
      "Training completed in 73.87 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer, set the number of epochs, and initiate training\n",
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5)\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
