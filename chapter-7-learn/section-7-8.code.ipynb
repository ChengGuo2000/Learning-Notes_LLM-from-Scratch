{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfa18db",
   "metadata": {},
   "source": [
    "# 7.8 Evaluating the fine-tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a7ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running\n",
    "import psutil\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88971852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the response data\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16cde9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for loading the response data\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b670d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying a local Ollama model\n",
    "import urllib.request\n",
    "def query_model(prompt, model=\"llama3\", url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"options\": {\"seed\": 123, \"temperature\": 0, \"num_ctx\": 2048}}\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "    \n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e9ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and digestive system.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# Test the querying function\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12c26b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a horse.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a horse.\" an 80 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to that of a horse.\n",
      "* The comparison is reasonable and understandable, as horses are known for their speed.\n",
      "* However, the model could have chosen a more vivid or unexpected comparison to make the sentence more engaging. For example, \"The car is as fast as a cheetah\" might be an even better simile.\n",
      "\n",
      "Overall, the response is good but not exceptional, which is why I'd give it an 80 out of 100.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a tropical rainforest.\n",
      "\n",
      "Score:\n",
      ">> I'd rate this model response a 0 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The instruction asks about the type of cloud associated with thunderstorms.\n",
      "* The model response completely misinterprets the question and provides an answer that has no relation to clouds or weather phenomena (tropical rainforest).\n",
      "* There is no attempt to provide any relevant information or context related to the original question.\n",
      "\n",
      "Overall, this response is not even close to being correct, which is why I'd give it a score of 0.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Robert Frost.\n",
      "\n",
      "Score:\n",
      ">> I'd rate this model response a 0 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The instruction asks for the author of \"Pride and Prejudice\", but the model responds with Robert Frost, who wrote a completely different book (\"The Road Not Taken\" is his most famous work).\n",
      "* This response is not only incorrect but also unrelated to the original question.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the first three examples from the test set\n",
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e53a5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the instruction fine-tuning LLM\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1196576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 10/10 [07:10<00:00, 43.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 10 of 110\n",
      "Average score: 49.90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the above function to the test data\n",
    "scores = generate_model_scores(test_data[10:20], \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "# I only used part of the test data since using the entire test data requires a long time on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "406c214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of Denmark?` and correct output `The capital of Denmark is Copenhagen.`, score the model response `The capital of Denmark is Copenhagen.`\n",
      "\n",
      ">>>The score is: 95\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the opposite of 'wet'?` and correct output `The opposite of 'wet' is 'dry'.`, score the model response `The opposite of 'wet' is 'dry'.`\n",
      "\n",
      ">>>The score is: 100\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the type of sentence.\n",
      "\n",
      "### Input:\n",
      "Did you finish the report?` and correct output `The type of sentence is interrogative.`, score the model response `The type of sentence is a verb.`\n",
      "\n",
      ">>>The score is: 20\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the chemical symbol for mercury?` and correct output `Hg.`, score the model response `The chemical symbol for mercury is mercury.`\n",
      "\n",
      ">>>The score is: 20\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the process of active transport in cells.` and correct output `Active transport is the movement of molecules across a cell membrane from a region of lower concentration to a region of higher concentration, using energy in the form of ATP. This process is crucial for maintaining cellular homeostasis.`, score the model response `Transport is the process by which cells migrate from one cell to another.`\n",
      "\n",
      ">>>The score is: 4\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify the following items as either solid, liquid, or gas.\n",
      "\n",
      "### Input:\n",
      "Mercury, oxygen, wood` and correct output `Mercury - Liquid\n",
      "Oxygen - Gas\n",
      "Wood - Solid`, score the model response `Solid\n",
      "Liquid\n",
      "Gas`\n",
      "\n",
      ">>>The score is: 80\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert 3 kilometers to meters.` and correct output `3 kilometers is 3000 meters.`, score the model response `3 kilometers is 3 meters.`\n",
      "\n",
      ">>>The score is: 20\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the sentence to use an indefinite pronoun.\n",
      "\n",
      "### Input:\n",
      "Someone left a note.` and correct output `A note was left by someone.`, score the model response `Someone left a note.`\n",
      "\n",
      ">>>The score is: 60\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Provide a synonym for 'excited'.` and correct output `A synonym for 'excited' is 'thrilled'.`, score the model response `A synonym for 'excited' is 'excited'.`\n",
      "\n",
      ">>>The score is: 20\n",
      "-------------------------\n",
      "Given the input `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a sentence that follows the pattern: \"Never have I ever _____ without _____\"` and correct output `Never have I ever traveled without a map.`, score the model response `The pattern of the sentence is: \"Never have I ever _____ without _____.\"`\n",
      "\n",
      ">>>The score is: 80\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print out all the scores for test_data[10:20]\n",
    "for index, entry in enumerate(test_data[10:20]):\n",
    "    prompt = (f\"Given the input `{format_input(entry)}` \"\n",
    "              f\"and correct output `{entry['output']}`, \"\n",
    "              f\"score the model response `{entry['model_response']}`\")\n",
    "    print(prompt)\n",
    "    print(\"\\n>>>The score is:\", scores[index])\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a791853",
   "metadata": {},
   "source": [
    "## Exercise 7.4 - Page 247\n",
    "Since this exercise takes too long to train on my laptop, I decided not to finish it. According to the exercise solutions, this adjustment results in an equal performance with the current approach. Please refer to the appendix-E-learn folder for my learning notes on LoRA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
