{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74a8038",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "I am using \"The Pit and the Pendulum\" by Edgar Allan Poe (which is in the public domain) for text preprocessing to get some different results from using \"The Verdict\" by Edith Wharton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "42408e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of character: 33679\n",
      "I was sick—sick unto death with that long agony; and when they at length unbound me, and I was perm\n"
     ]
    }
   ],
   "source": [
    "with open(\"pit-and-pendulum.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"total number of character: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3f86516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome,', ' ', 'everyone.', ' ', 'My', ' ', 'name,', ' ', 'is', ' ', 'Cheng', ' ', 'Guo.']\n"
     ]
    }
   ],
   "source": [
    "# Split a text on whitespace characters\n",
    "import re\n",
    "text = \"Welcome, everyone. My name, is Cheng Guo.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "275d0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', ',', '', ' ', 'everyone', '.', '', ' ', 'My', ' ', 'name', ',', '', ' ', 'is', ' ', 'Cheng', ' ', 'Guo', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# Split on whitespaces, commas, and periods\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3bf5e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', ',', 'everyone', '.', 'My', 'name', ',', 'is', 'Cheng', 'Guo', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove redundant whitespace characters\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c6bd3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', ',', 'everyone', '!', 'Am', 'I', 'Cheng', '--', 'Guo', '?']\n"
     ]
    }
   ],
   "source": [
    "# Adjust to handle other types of punctuation\n",
    "text = \"Wow, everyone! Am I Cheng--Guo?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00fc024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7061\n",
      "['I', 'was', 'sick', '—', 'sick', 'unto', 'death', 'with', 'that', 'long', 'agony', ';', 'and', 'when', 'they', 'at', 'length', 'unbound', 'me', ',', 'and', 'I', 'was', 'permitted', 'to', 'sit', ',', 'I', 'felt', 'that']\n"
     ]
    }
   ],
   "source": [
    "# PRocessing the short story\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e1540",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4a7908e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680\n"
     ]
    }
   ],
   "source": [
    "# List and sort all unique tokens\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f74f1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('.', 6)\n",
      "(':', 7)\n",
      "(';', 8)\n",
      "('?', 9)\n",
      "('A', 10)\n",
      "('After', 11)\n",
      "('Agitation', 12)\n",
      "('All', 13)\n",
      "('Amid', 14)\n",
      "('An', 15)\n",
      "('And', 16)\n",
      "('Another', 17)\n",
      "('Arousing', 18)\n",
      "('As', 19)\n",
      "('At', 20)\n",
      "('Avoiding', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Could', 24)\n",
      "('Days', 25)\n",
      "('Death', 26)\n",
      "('Demon', 27)\n",
      "('Down', 28)\n",
      "('Dreading', 29)\n",
      "('During', 30)\n",
      "('Else', 31)\n",
      "('Even', 32)\n",
      "('Fate', 33)\n",
      "('Fool', 34)\n",
      "('For', 35)\n",
      "('Forth', 36)\n",
      "('Free', 37)\n",
      "('French', 38)\n",
      "('From', 39)\n",
      "('General', 40)\n",
      "('Groping', 41)\n",
      "('Had', 42)\n",
      "('Hades', 43)\n",
      "('Having', 44)\n",
      "('He', 45)\n",
      "('How', 46)\n",
      "('I', 47)\n",
      "('In', 48)\n",
      "('Inch', 49)\n",
      "('Inquisition', 50)\n"
     ]
    }
   ],
   "source": [
    "# Creating a vocabulary\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "53ac04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a simple text tokenizer\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} # creates the inverse vocab\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', text) # processes input text into IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\']|--|—|\\s)', r'\\1', text) # removes spaces before the specified punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bb9cc29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 1259, 1464, 1346, 1455, 1043, 1542, 1600, 1668, 269, 1465, 1377, 803, 118, 366, 1656, 979, 1237, 1679, 151, 1656, 1479, 1017, 1470, 1413, 290, 1054, 979, 1361, 136, 1465, 848, 5, 342, 289, 1030, 439, 6]\n"
     ]
    }
   ],
   "source": [
    "# Try the tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"I saw that some ten or twelve vibrations would bring the steel \n",
    "            in actual contact with my robe—and with this observation there suddenly \n",
    "            came over my spirit all the keen, collected calmness of despair.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb1ceb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw that some ten or twelve vibrations would bring the steel in actual contact with my robe— and with this observation there suddenly came over my spirit all the keen, collected calmness of despair.\n"
     ]
    }
   ],
   "source": [
    "# Convert back\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bfe610ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This error is a KeyError on the word 'Hello'\n"
     ]
    }
   ],
   "source": [
    "# New text (modified code)\n",
    "try:\n",
    "    text = \"Hello, do you like tea?\"\n",
    "    print(tokenizer.encode(text))\n",
    "except Exception as e:\n",
    "    print(f\"This error is a {type(e).__name__} on the word {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e4343",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1b6ba66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fce8ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('yawning', 1677)\n",
      "('yet', 1678)\n",
      "('—', 1679)\n",
      "('<|endoftext|>', 1680)\n",
      "('<|unk|>', 1681)\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76003a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text tokenizer\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} # creates the inverse vocab\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', text) # processes input text into IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\']|--|—|\\s)', r'\\1', text) # removes spaces before the specified punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7e5c4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> This is a story about pit and pendulum.\n"
     ]
    }
   ],
   "source": [
    "# Try the new tokenizer\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"This is a story about pit and pendulum.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "196263c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1681, 5, 474, 1681, 881, 1681, 9, 1680, 85, 837, 103, 1681, 104, 1099, 151, 1077, 6]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sample text\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a10bcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do <|unk|> like <|unk|>? <|endoftext|> This is a <|unk|> about pit and pendulum.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
