{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74a8038",
   "metadata": {},
   "source": [
    "# 2.2 - Tokenizing text\n",
    "I am using \"The Pit and the Pendulum\" by Edgar Allan Poe (which is in the public domain) instead of \"The Verdict\" to get some novel results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42408e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of character: 33679\n",
      "I was sick—sick unto death with that long agony; and when they at length unbound me, and I was perm\n"
     ]
    }
   ],
   "source": [
    "# Import text\n",
    "with open(\"pit-and-pendulum.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(f\"total number of character: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f86516d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome,', ' ', 'everyone.', ' ', 'My', ' ', 'name,', ' ', 'is', ' ', 'Cheng', ' ', 'Guo.']\n"
     ]
    }
   ],
   "source": [
    "# Split a text on whitespace characters\n",
    "import re\n",
    "text = \"Welcome, everyone. My name, is Cheng Guo.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "275d0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', ',', '', ' ', 'everyone', '.', '', ' ', 'My', ' ', 'name', ',', '', ' ', 'is', ' ', 'Cheng', ' ', 'Guo', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# Split on whitespaces, commas, and periods\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bf5e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', ',', 'everyone', '.', 'My', 'name', ',', 'is', 'Cheng', 'Guo', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove redundant whitespace characters\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6bd3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', ',', 'everyone', '!', 'Am', 'I', 'Cheng', '--', 'Guo', '?']\n"
     ]
    }
   ],
   "source": [
    "# Adjust to handle other types of punctuation\n",
    "text = \"Wow, everyone! Am I Cheng--Guo?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00fc024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7061\n",
      "['I', 'was', 'sick', '—', 'sick', 'unto', 'death', 'with', 'that', 'long', 'agony', ';', 'and', 'when', 'they', 'at', 'length', 'unbound', 'me', ',', 'and', 'I', 'was', 'permitted', 'to', 'sit', ',', 'I', 'felt', 'that']\n"
     ]
    }
   ],
   "source": [
    "# PRocessing the short story\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e1540",
   "metadata": {},
   "source": [
    "# 2.3 - Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a7908e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680\n"
     ]
    }
   ],
   "source": [
    "# List and sort all unique tokens\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f74f1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('.', 6)\n",
      "(':', 7)\n",
      "(';', 8)\n",
      "('?', 9)\n",
      "('A', 10)\n",
      "('After', 11)\n",
      "('Agitation', 12)\n",
      "('All', 13)\n",
      "('Amid', 14)\n",
      "('An', 15)\n",
      "('And', 16)\n",
      "('Another', 17)\n",
      "('Arousing', 18)\n",
      "('As', 19)\n",
      "('At', 20)\n",
      "('Avoiding', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Could', 24)\n",
      "('Days', 25)\n",
      "('Death', 26)\n",
      "('Demon', 27)\n",
      "('Down', 28)\n",
      "('Dreading', 29)\n",
      "('During', 30)\n",
      "('Else', 31)\n",
      "('Even', 32)\n",
      "('Fate', 33)\n",
      "('Fool', 34)\n",
      "('For', 35)\n",
      "('Forth', 36)\n",
      "('Free', 37)\n",
      "('French', 38)\n",
      "('From', 39)\n",
      "('General', 40)\n",
      "('Groping', 41)\n",
      "('Had', 42)\n",
      "('Hades', 43)\n",
      "('Having', 44)\n",
      "('He', 45)\n",
      "('How', 46)\n",
      "('I', 47)\n",
      "('In', 48)\n",
      "('Inch', 49)\n",
      "('Inquisition', 50)\n"
     ]
    }
   ],
   "source": [
    "# Creating a vocabulary\n",
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53ac04b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a simple text tokenizer\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} # creates the inverse vocab\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', text) # processes input text into IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\']|--|—|\\s)', r'\\1', text) # removes spaces before the specified punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb9cc29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 1259, 1464, 1346, 1455, 1043, 1542, 1600, 1668, 269, 1465, 1377, 803, 118, 366, 1656, 979, 1237, 1679, 151, 1656, 1479, 1017, 1470, 1413, 290, 1054, 979, 1361, 136, 1465, 848, 5, 342, 289, 1030, 439, 6]\n"
     ]
    }
   ],
   "source": [
    "# Try the tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"I saw that some ten or twelve vibrations would bring the steel \n",
    "            in actual contact with my robe—and with this observation there suddenly \n",
    "            came over my spirit all the keen, collected calmness of despair.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb1ceb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw that some ten or twelve vibrations would bring the steel in actual contact with my robe— and with this observation there suddenly came over my spirit all the keen, collected calmness of despair.\n"
     ]
    }
   ],
   "source": [
    "# Convert back\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfe610ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This error is a KeyError on the word 'Hello'\n"
     ]
    }
   ],
   "source": [
    "# New text (modified code)\n",
    "try:\n",
    "    text = \"Hello, do you like tea?\"\n",
    "    print(tokenizer.encode(text))\n",
    "except Exception as e:\n",
    "    print(f\"This error is a {type(e).__name__} on the word {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e4343",
   "metadata": {},
   "source": [
    "# 2.4 - Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b6ba66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fce8ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('yawning', 1677)\n",
      "('yet', 1678)\n",
      "('—', 1679)\n",
      "('<|endoftext|>', 1680)\n",
      "('<|unk|>', 1681)\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76003a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New text tokenizer\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # stores vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} # creates the inverse vocab\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|—|\\s)', text) # processes input text into IDs\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\']|--|—|\\s)', r'\\1', text) # removes spaces before the specified punctuation\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e5c4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> This is a story about pit and pendulum.\n"
     ]
    }
   ],
   "source": [
    "# Try the new tokenizer\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"This is a story about pit and pendulum.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "196263c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1681, 5, 474, 1681, 881, 1681, 9, 1680, 85, 837, 103, 1681, 104, 1099, 151, 1077, 6]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sample text\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a10bcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do <|unk|> like <|unk|>? <|endoftext|> This is a <|unk|> about pit and pendulum.\n"
     ]
    }
   ],
   "source": [
    "# Detokenize\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6b40c",
   "metadata": {},
   "source": [
    "# 2.5 - Byte pair encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fffab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tiktoken\n",
    "# !uv pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5df7c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "# Import Tiktoken\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b76e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \" of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "938bc7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# Convert back\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28306668",
   "metadata": {},
   "source": [
    "## Exercise 2.1 - Page 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c455099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "# BPE for unknown words\n",
    "unknown = \"Akwirw ier\"\n",
    "code = tokenizer.encode(unknown)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2cdce66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ak\" -> 33901\n",
      "\"w\" -> 86\n",
      "\"ir\" -> 343\n",
      "\"w\" -> 86\n",
      "\" \" -> 220\n",
      "\"ier\" -> 959\n"
     ]
    }
   ],
   "source": [
    "# Decode each integer\n",
    "for integer in code:\n",
    "    print(f\"\\\"{tokenizer.decode([integer])}\\\"\" + \" -> \" + str(integer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39dc3fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "# Reconstruction\n",
    "unknown_str = tokenizer.decode(code)\n",
    "print(unknown_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a64211",
   "metadata": {},
   "source": [
    "# 2.6 - Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d028f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7702\n"
     ]
    }
   ],
   "source": [
    "# Create BPE tokenizer for the short story\n",
    "with open(\"pit-and-pendulum.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "encode_text = tokenizer.encode(raw_text)\n",
    "print(len(encode_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f527b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [286, 7310, 18702, 2288]\n",
      "y:      [7310, 18702, 2288, 543]\n"
     ]
    }
   ],
   "source": [
    "# Create the input-target pairs\n",
    "encode_sample = encode_text[50:]\n",
    "context_size = 4\n",
    "x = encode_sample[:context_size]\n",
    "y = encode_sample[1:context_size + 1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a4635d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[286] -----> 7310\n",
      "[286, 7310] -----> 18702\n",
      "[286, 7310, 18702] -----> 2288\n",
      "[286, 7310, 18702, 2288] -----> 543\n"
     ]
    }
   ],
   "source": [
    "# Create the next-word prediction tasks\n",
    "for i in range(1, context_size + 1):\n",
    "    context = encode_sample[:i]\n",
    "    desired = encode_sample[i]\n",
    "    print(context, \"----->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6506174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of ----->  distinct\n",
      " of distinct ----->  accent\n",
      " of distinct accent -----> uation\n",
      " of distinct accentuation ----->  which\n"
     ]
    }
   ],
   "source": [
    "# Repeat with decoded text\n",
    "for i in range(1, context_size + 1):\n",
    "    context = encode_sample[:i]\n",
    "    desired = encode_sample[i]\n",
    "    print(tokenizer.decode(context), \"----->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9fbae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient data loader implementation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0, len(token_ids) - max_length, stride): # Use sliding window to chunk the text\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9d1fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data loader\n",
    "def create_dataloader_v1(text, batch_size = 4, max_length = 256, \n",
    "                         stride = 128, shuffle = True, drop_last = True, num_workers =  0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ade08466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  373, 6639,  960]]), tensor([[ 373, 6639,  960,   82]])]\n"
     ]
    }
   ],
   "source": [
    "# Test the data loader\n",
    "with open(\"pit-and-pendulum.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_length = 4, stride = 1, shuffle = False)\n",
    "data_iterator = iter(dataloader)\n",
    "first_batch = next(data_iterator)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8066fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 373, 6639,  960,   82]]), tensor([[6639,  960,   82,  624]])]\n"
     ]
    }
   ],
   "source": [
    "# Fetch the next batch\n",
    "second_batch = next(data_iterator)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f195a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   373,  6639,   960],\n",
      "        [   82,   624, 12722,  1918],\n",
      "        [  351,   326,   890, 35358],\n",
      "        [   26,   290,   618,   484],\n",
      "        [  379,  4129,   555,  7784],\n",
      "        [  502,    11,   290,   314],\n",
      "        [  373, 10431,   284,  1650],\n",
      "        [   11,   314,  2936,   326]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  373,  6639,   960,    82],\n",
      "        [  624, 12722,  1918,   351],\n",
      "        [  326,   890, 35358,    26],\n",
      "        [  290,   618,   484,   379],\n",
      "        [ 4129,   555,  7784,   502],\n",
      "        [   11,   290,   314,   373],\n",
      "        [10431,   284,  1650,    11],\n",
      "        [  314,  2936,   326,   616]])\n"
     ]
    }
   ],
   "source": [
    "# Batch size greater than 1\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8, max_length = 4, stride = 4, shuffle = False)\n",
    "data_iterator = iter(dataloader)\n",
    "inputs, targets = next(data_iterator)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08c3c7",
   "metadata": {},
   "source": [
    "## Exercise 2.2 - Page 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b41985e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 40, 373]]), tensor([[ 373, 6639]])]\n",
      "[tensor([[6639,  960]]), tensor([[960,  82]])]\n",
      "[tensor([[   40,   373,  6639,   960,    82,   624, 12722,  1918]]), tensor([[  373,  6639,   960,    82,   624, 12722,  1918,   351]])]\n",
      "[tensor([[ 6639,   960,    82,   624, 12722,  1918,   351,   326]]), tensor([[  960,    82,   624, 12722,  1918,   351,   326,   890]])]\n"
     ]
    }
   ],
   "source": [
    "# Different stride and context sizes\n",
    "dataloader_2 = create_dataloader_v1(raw_text, batch_size = 1, max_length = 2, stride = 2, shuffle = False)\n",
    "data_iterator_2 = iter(dataloader_2)\n",
    "first_batch_2 = next(data_iterator_2)\n",
    "second_batch_2 = next(data_iterator_2)\n",
    "print(first_batch_2)\n",
    "print(second_batch_2)\n",
    "dataloader_3 = create_dataloader_v1(raw_text, batch_size = 1, max_length = 8, stride = 2, shuffle = False)\n",
    "data_iterator_3 = iter(dataloader_3)\n",
    "first_batch_3 = next(data_iterator_3)\n",
    "second_batch_3 = next(data_iterator_3)\n",
    "print(first_batch_3)\n",
    "print(second_batch_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9a75b",
   "metadata": {},
   "source": [
    "# 2.7 - Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9b6e358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Convert token ID to embedding vector\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f25acd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test embedding vector\n",
    "print(embedding_layer(torch.tensor([3]))) # this is identical to the fourth row above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4069c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Convert all four input IDs\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffc34a",
   "metadata": {},
   "source": [
    "# 2.8 - Encoding word positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9f3ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   373,  6639,   960],\n",
      "        [   82,   624, 12722,  1918],\n",
      "        [  351,   326,   890, 35358],\n",
      "        [   26,   290,   618,   484],\n",
      "        [  379,  4129,   555,  7784],\n",
      "        [  502,    11,   290,   314],\n",
      "        [  373, 10431,   284,  1650],\n",
      "        [   11,   314,  2936,   326]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# Use larger embedding sizes\n",
    "vocab_size = 50257 # BPE tokenizer\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8, max_length = max_length, stride = max_length, shuffle = False)\n",
    "data_iterator = iter(dataloader)\n",
    "inputs, targets = next(data_iterator)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8affe575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Embed the token IDs\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "687f889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Absolute positional embedding\n",
    "context_length = max_length\n",
    "position_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "position_embeddings = position_embedding_layer(torch.arange(context_length))\n",
    "print(position_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "60bebb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Add the position embeddings to the token embeddings\n",
    "input_embeddings = token_embeddings + position_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning-Notes_LLM-from-Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
