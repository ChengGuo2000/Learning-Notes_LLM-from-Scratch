# Chapter 5 Reading Notes

## Evaluating generative text models
- **Weight Parameters** are stored in linear layers, which are utilized in multi-head attention and GPT model. After initializing a layer (`new_layer = torch.nn.Linear(...)`), we can access its weights through the `.weight` attribute, `new_layer.weight`. Additionaly, we can also access all the trainable parameters of a model through method `model.parameters()`. The weight update is done via **backpropagation**.
- To define what makes text "coherent" or "high quality", we have to implement a numerical method to evaluate the generated content. This approach will enable us to monitor and enhance hte model's performance throughout its training process. We will also calculate a **loss metric** for the generated outputs, which serves as a progress and success indicator of the training progress.
- Part of the text evaluation process is to meausre "**how far**" the generated tokens are from the correct predictions (targets). The training function will use this information to adjust the model weights to generate text that is more similar to (or ideally matches) the target text.
- The model training aims to increase the softmax probability in the index positions corresponding to the correct target token IDs. This **softmax** probability is also used in the evaluation metric to numerically assess the model's generated outputs: the higher the probability in the correct positions, the better.
- The **goal** of training an LLM is to maximize the likelihood of the correct token, which involves increasing its probability relative to other tokens. Calculate the loss involves: logits to probabilities, to target probabilities, to log probabilities, to average log probability, to negative average log probability.
- The **Cross Entropy Loss** is a popular measure that measures the difference between two probability distributions. The `cross_entropy` function computes this measure for discrete outcomes, which is similar to the negative average log probability of the target tokens given the model's generated token probabilities. **Cross Entropy** and **Negative Average Log Probability** are often used interchangeably in practice.
- **Perplexity** is a meausre using alongside cross entropy loss that provides a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence. It measures how well the distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution. It can be calculated as `perplexity = torch.exp(loss)`. It is more interpretable because it signifies the effective vocabulary size about which the model is uncertain at each step.
- The cost of training a 7 billion parameter Llama 2 model requaires 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion tokens. Running 8 * A100 cloud server on AWS costs around $30 per hour, so the total training cost is around $690,000.

## Training an LLM
- There are **eight** steps, starting with iterating over each epoch, processing batches, resetting gradients, calculating the loss and new gradients, and updating weights and concluding with monitoring steps like printing losses and generating text samples. One **epoch** is one complete pass over a training set. The number of batches is determined by the training set size divided by the size of each batch.
- **Adam** optimizers are a popular choice for training DNN. We selected **AdamW** optimizer, which is a variant of **Adam** that improves the weight decay approach, which aims to minimize model complexity and prevent overfitting by penalizing larger weights. This adjustment allows **AdamW** to achieve more effective regularization and better generalization, thus it is frequently used in training LLMs.
- Model memorizing the data is expected when working with a very, very small training dataset and training the model for multiple epochs. Usually, it is common to train a model on a much larger dataset for only one epoch.

## Useful Links