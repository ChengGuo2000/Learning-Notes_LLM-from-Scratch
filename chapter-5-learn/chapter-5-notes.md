# Chapter 5 Reading Notes

## Evaluating generative text models
- **Weight Parameters** are stored in linear layers, which are utilized in multi-head attention and GPT model. After initializing a layer (`new_layer = torch.nn.Linear(...)`), we can access its weights through the `.weight` attribute, `new_layer.weight`. Additionaly, we can also access all the trainable parameters of a model through method `model.parameters()`. The weight update is done via **backpropagation**.
- To define what makes text "coherent" or "high quality", we have to implement a numerical method to evaluate the generated content. This approach will enable us to monitor and enhance hte model's performance throughout its training process. We will also calculate a **loss metric** for the generated outputs, which serves as a progress and success indicator of the training progress.
- Part of the text evaluation process is to meausre "**how far**" the generated tokens are from the correct predictions (targets). The training function will use this information to adjust the model weights to generate text that is more similar to (or ideally matches) the target text.
- The model training aims to increase the softmax probability in the index positions corresponding to the correct target token IDs. This **softmax** probability is also used in the evaluation metric to numerically assess the model's generated outputs: the higher the probability in the correct positions, the better.
- The **goal** of training an LLM is to maximize the likelihood of the correct token, which involves increasing its probability relative to other tokens. Calculate the loss involves: logits to probabilities, to target probabilities, to log probabilities, to average log probability, to negative average log probability.
- The **Cross Entropy Loss** is a popular measure that measures the difference between two probability distributions. The `cross_entropy` function computes this measure for discrete outcomes, which is similar to the negative average log probability of the target tokens given the model's generated token probabilities. **Cross Entropy** and **Negative Average Log Probability** are often used interchangeably in practice.
- **Perplexity** is a meausre using alongside cross entropy loss that provides a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence. It measures how well the distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution. It can be calculated as `perplexity = torch.exp(loss)`. It is more interpretable because it signifies the effective vocabulary size about which the model is uncertain at each step.
- The cost of training a 7 billion parameter Llama 2 model requaires 184,320 GPU hours on expensive A100 GPUs, processing 2 trillion tokens. Running 8 * A100 cloud server on AWS costs around $30 per hour, so the total training cost is around $690,000.

## Training an LLM
- There are **eight** steps, starting with iterating over each epoch, processing batches, resetting gradients, calculating the loss and new gradients, and updating weights and concluding with monitoring steps like printing losses and generating text samples. One **epoch** is one complete pass over a training set. The number of batches is determined by the training set size divided by the size of each batch.
- Model **memorizing** the data is expected when working with a very, very small training dataset and training the model for multiple epochs. Usually, it is common to train a model on a much larger dataset for only one epoch.
- **Adam** optimizers are a popular choice for training DNN. We selected **AdamW** optimizer, which is a variant of **Adam** that improves the weight decay approach, which aims to minimize model complexity and prevent overfitting by penalizing larger weights. This adjustment allows **AdamW** to achieve more effective regularization and better generalization, thus it is frequently used in training LLMs.
- Adaptive optimizers such as **AdamW** store additional parameters for each model weight. **AdamW** uses historical data to adjust learning rates for each model parameter dynamically. Without it, the optimizer resets, and the model may learn suboptimally or even fail to converge properly, which means it will lose the ability to generate coherent text. So, when saving a model's state_dict, we should also save the state_dict of the optimizer for future pretraining.

## Decoding strategies to control randomness
- **Decoding Stratigies** reduce training data memorization and increase the originality of the LLM-generated text. Two approaches are **temperature scaling** and **top-k sampling**.
- For **Greedy Decoding**, we always sampled the token with the highest probability as the next token using `torch.argmax`, which means that the LLM will always generate the same outputs on the same context even if we try to generate multiple times.
- We can replace `argmax` with a function that samples from a probability distribution using the `multinomial` function. We can further control the distribution and selection process via a concept called **temperature scaling**.
- **Temperature scaling** divides the logits by a number greater than 0.
    - **Temperatures** greater than 1 result in more uniformly distributed token probabilities, resulting in a distribution similar to uniform distribution, which leads to more diverse output, but may contain grammar mistakes.
    - **Temperatures** smaller than 1 result in more confident (sharper or peaky) distributions, approaching the behavior of the `argmax` function.
    - **Temperature** equal to 1 is the same as not using any temperature scaling. 
- **Top-k Sampling** combined with probabilistic sampling and temperature scaling can improve the text generation results. In **top-k sampling**, we can restrict the sampled tokens to the top-k most likely tokens and exclude all other tokens from the selection process by masking their probability scores with a `-inf` mask. It replaces all the nonselected logits with negative infinity, so when computing softmax, the probability scores of all non-top-k tokens are 0 and the probabilities of top-k tokens sum up to 1.
- Lower temperature and lower top-k can be applied when we need an accurate answer, like generating code or official documents. Higher temperature and higher top-k can be applied when we need creativity, like writing fictions. We can force deterministic behavior through settting top_k to None or 1 and setting temperature to None or 0.

## Useful Links
- [LLM Visualization](https://bbycroft.net/llm)